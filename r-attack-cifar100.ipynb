{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference attack on ResNet-18 models for CIFAR-100 classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by installing the required libraries, and importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXpmi98p6SEy",
    "outputId": "37d89404-a3d7-4af7-bad3-0b28038a7fdc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install /mnt/backup1/home/victor/ml_privacy_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xwB2mwTiDT8v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch import nn, optim, Tensor\n",
    "import tensorflow as tf\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fyqac8f68Gjt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from privacy_meter.audit import Audit, MetricEnum\n",
    "from privacy_meter.audit_report import ROCCurveReport, SignalHistogramReport\n",
    "from privacy_meter.constants import InferenceGame\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.model import PytorchModel\n",
    "from privacy_meter.hypothesis_test import threshold_func, gaussian_threshold_func\n",
    "from privacy_meter.metric import ReferenceMetric\n",
    "from privacy_meter.information_source_signal import ModelLoss\n",
    "from privacy_meter import audit_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fix the seed for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4h_prRIZDFA4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the hyperparameters for both the training of the models and the reference attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xfGRanuPDLif",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_points_per_train_split = 15_000\n",
    "num_points_per_test_split = 5_000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "num_reference_models = 24\n",
    "fpr_tolerance_list = np.linspace(0, 1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download and preprocess the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O8sCZ6FkECSO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_cifar100_dataset():\n",
    "    input_shape, num_classes = (32, 32, 3), 100\n",
    "\n",
    "    # split the data between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "    # scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # switch channels to meet pytorch expectations\n",
    "    x_train = np.moveaxis(x_train, -1, 1)\n",
    "    x_test = np.moveaxis(x_test, -1, 1)\n",
    "\n",
    "    y_train = y_train[:, 0]\n",
    "    y_test = y_test[:, 0]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, input_shape, num_classes\n",
    "\n",
    "x_train_all, y_train_all, x_test_all, y_test_all, input_shape, num_classes = preprocess_cifar100_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And wrap it in the *Dataset* object from *privacy-meter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FlALwVdvD-kh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    data_dict={\n",
    "        'train': {'x': x_train_all, 'y': y_train_all},\n",
    "        'test': {'x': x_test_all, 'y': y_test_all}\n",
    "    },\n",
    "    default_input='x',\n",
    "    default_output='y'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a built-in function to create a target subset, and reference subsets. Using the hybrid method, there is no overlap between the target target subset and the reference subsets, but overlap between two reference subsets is authorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lxgx5mAuEMAi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_list = dataset.subdivide(\n",
    "    num_splits=num_reference_models + 1,\n",
    "    delete_original=True,\n",
    "    in_place=True,\n",
    "    return_results=True,\n",
    "    method='hybrid',\n",
    "    split_size={'train': num_points_per_train_split, 'test': num_points_per_test_split}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model architecture, in that case ResNet-18. Note that we **do not** use a pretrained version, to control members vs non-members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JB91jreKGaGZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model_ft = models.resnet18(weights=None)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a boilerplate PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xM2-ChuJGuqE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, k, device):\n",
    "  model = model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "  x_train = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_input>')\n",
    "  y_train = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_output>')\n",
    "  x_test = Tensor(dataset.get_feature(split_name=f'test{k:03d}', feature_name='<default_input>')).to(device)\n",
    "  y_test = Tensor(dataset.get_feature(split_name=f'test{k:03d}', feature_name='<default_output>')).to(int).to(device)\n",
    "  n_samples = x_train.shape[0]\n",
    "  n_batches = ceil(n_samples / batch_size)\n",
    "  x_train = np.array_split(x_train, n_batches)\n",
    "  y_train = np.array_split(y_train, n_batches)\n",
    "  for epoch in range(epochs):\n",
    "      epoch_loss, acc = 0.0, 0.0\n",
    "      for b in range(n_batches):\n",
    "          optimizer.zero_grad()\n",
    "          y_pred = model(Tensor(x_train[b]).to(device))\n",
    "          loss = loss_fn(y_pred, Tensor(y_train[b]).to(int).to(device))\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          epoch_loss += loss.item()\n",
    "          acc += torch.sum(y_pred.argmax(axis=1) == Tensor(y_train[b]).to(int).to(device))\n",
    "      acc /= n_samples\n",
    "      epoch_loss /= n_samples\n",
    "      y_pred = model(x_test)\n",
    "      test_loss = loss_fn(y_pred, y_test)\n",
    "      test_acc = torch.sum(y_pred.argmax(axis=1) == y_test) / x_test.shape[0]\n",
    "      print(f'model #{k:02d}, epoch #{epoch:03d}:   train_acc={100*acc:05.1f}%   train_loss={epoch_loss:.2e}   test_acc={100*test_acc:05.1f}%   test_loss={test_loss:.2e}')\n",
    "  return model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now execute the previously defined model-related functions, and wrap the results in *PytorchModel* objects from *privacy-meter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUX65b8UJmtZ",
    "outputId": "9510e652-609b-4fdd-e25e-8c7d5ccad97b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model #00, epoch #000:   train_acc=006.1%   train_loss=1.73e-02   test_acc=010.6%   test_loss=4.04e+00\n",
      "model #00, epoch #001:   train_acc=020.8%   train_loss=1.39e-02   test_acc=014.0%   test_loss=3.78e+00\n",
      "model #00, epoch #002:   train_acc=038.4%   train_loss=1.14e-02   test_acc=016.0%   test_loss=3.67e+00\n",
      "model #00, epoch #003:   train_acc=057.4%   train_loss=9.00e-03   test_acc=016.9%   test_loss=3.63e+00\n",
      "model #00, epoch #004:   train_acc=073.6%   train_loss=6.89e-03   test_acc=017.0%   test_loss=3.64e+00\n",
      "model #00, epoch #005:   train_acc=082.0%   train_loss=5.46e-03   test_acc=016.8%   test_loss=3.67e+00\n",
      "model #00, epoch #006:   train_acc=086.8%   train_loss=4.44e-03   test_acc=016.8%   test_loss=3.71e+00\n",
      "model #00, epoch #007:   train_acc=090.8%   train_loss=3.56e-03   test_acc=016.5%   test_loss=3.78e+00\n",
      "model #00, epoch #008:   train_acc=094.6%   train_loss=2.69e-03   test_acc=016.5%   test_loss=3.87e+00\n",
      "model #00, epoch #009:   train_acc=096.9%   train_loss=1.96e-03   test_acc=016.3%   test_loss=3.92e+00\n",
      "model #00, epoch #010:   train_acc=098.7%   train_loss=1.35e-03   test_acc=016.9%   test_loss=3.97e+00\n",
      "model #00, epoch #011:   train_acc=099.5%   train_loss=8.89e-04   test_acc=016.5%   test_loss=4.02e+00\n",
      "model #00, epoch #012:   train_acc=099.9%   train_loss=5.93e-04   test_acc=016.4%   test_loss=4.07e+00\n",
      "model #00, epoch #013:   train_acc=099.9%   train_loss=4.19e-04   test_acc=016.9%   test_loss=4.10e+00\n",
      "model #00, epoch #014:   train_acc=099.9%   train_loss=2.79e-04   test_acc=016.8%   test_loss=4.13e+00\n",
      "model #00, epoch #015:   train_acc=100.0%   train_loss=1.86e-04   test_acc=017.2%   test_loss=4.16e+00\n",
      "model #00, epoch #016:   train_acc=100.0%   train_loss=1.26e-04   test_acc=017.4%   test_loss=4.17e+00\n",
      "model #00, epoch #017:   train_acc=100.0%   train_loss=8.48e-05   test_acc=017.5%   test_loss=4.18e+00\n",
      "model #00, epoch #018:   train_acc=100.0%   train_loss=6.31e-05   test_acc=017.9%   test_loss=4.18e+00\n",
      "model #00, epoch #019:   train_acc=100.0%   train_loss=5.00e-05   test_acc=017.9%   test_loss=4.20e+00\n",
      "model #00, epoch #020:   train_acc=100.0%   train_loss=4.26e-05   test_acc=017.7%   test_loss=4.21e+00\n",
      "model #00, epoch #021:   train_acc=100.0%   train_loss=3.71e-05   test_acc=017.8%   test_loss=4.22e+00\n",
      "model #00, epoch #022:   train_acc=100.0%   train_loss=3.36e-05   test_acc=017.8%   test_loss=4.23e+00\n",
      "model #00, epoch #023:   train_acc=100.0%   train_loss=3.05e-05   test_acc=017.8%   test_loss=4.24e+00\n",
      "model #00, epoch #024:   train_acc=100.0%   train_loss=2.80e-05   test_acc=017.9%   test_loss=4.25e+00\n",
      "model #00, epoch #025:   train_acc=100.0%   train_loss=2.58e-05   test_acc=017.8%   test_loss=4.25e+00\n",
      "model #00, epoch #026:   train_acc=100.0%   train_loss=2.40e-05   test_acc=017.9%   test_loss=4.26e+00\n",
      "model #00, epoch #027:   train_acc=100.0%   train_loss=2.24e-05   test_acc=017.9%   test_loss=4.27e+00\n",
      "model #00, epoch #028:   train_acc=100.0%   train_loss=2.09e-05   test_acc=017.9%   test_loss=4.28e+00\n",
      "model #00, epoch #029:   train_acc=100.0%   train_loss=1.96e-05   test_acc=017.9%   test_loss=4.29e+00\n",
      "model #00, epoch #030:   train_acc=100.0%   train_loss=1.85e-05   test_acc=018.1%   test_loss=4.29e+00\n",
      "model #00, epoch #031:   train_acc=100.0%   train_loss=1.74e-05   test_acc=017.9%   test_loss=4.30e+00\n",
      "model #00, epoch #032:   train_acc=100.0%   train_loss=1.65e-05   test_acc=017.9%   test_loss=4.31e+00\n",
      "model #00, epoch #033:   train_acc=100.0%   train_loss=1.56e-05   test_acc=017.9%   test_loss=4.32e+00\n",
      "model #00, epoch #034:   train_acc=100.0%   train_loss=1.48e-05   test_acc=018.0%   test_loss=4.32e+00\n",
      "model #00, epoch #035:   train_acc=100.0%   train_loss=1.40e-05   test_acc=018.0%   test_loss=4.33e+00\n",
      "model #00, epoch #036:   train_acc=100.0%   train_loss=1.33e-05   test_acc=018.0%   test_loss=4.34e+00\n",
      "model #00, epoch #037:   train_acc=100.0%   train_loss=1.27e-05   test_acc=018.0%   test_loss=4.34e+00\n",
      "model #00, epoch #038:   train_acc=100.0%   train_loss=1.20e-05   test_acc=018.0%   test_loss=4.35e+00\n",
      "model #00, epoch #039:   train_acc=100.0%   train_loss=1.15e-05   test_acc=017.9%   test_loss=4.36e+00\n",
      "model #00, epoch #040:   train_acc=100.0%   train_loss=1.11e-05   test_acc=017.9%   test_loss=4.37e+00\n",
      "model #00, epoch #041:   train_acc=100.0%   train_loss=1.06e-05   test_acc=018.0%   test_loss=4.37e+00\n",
      "model #00, epoch #042:   train_acc=100.0%   train_loss=1.02e-05   test_acc=018.2%   test_loss=4.38e+00\n",
      "model #00, epoch #043:   train_acc=100.0%   train_loss=9.72e-06   test_acc=018.0%   test_loss=4.39e+00\n",
      "model #00, epoch #044:   train_acc=100.0%   train_loss=9.42e-06   test_acc=018.1%   test_loss=4.39e+00\n",
      "model #00, epoch #045:   train_acc=100.0%   train_loss=9.06e-06   test_acc=017.9%   test_loss=4.40e+00\n",
      "model #00, epoch #046:   train_acc=100.0%   train_loss=8.93e-06   test_acc=018.0%   test_loss=4.41e+00\n",
      "model #00, epoch #047:   train_acc=100.0%   train_loss=8.32e-06   test_acc=017.9%   test_loss=4.41e+00\n",
      "model #00, epoch #048:   train_acc=100.0%   train_loss=8.33e-06   test_acc=018.0%   test_loss=4.42e+00\n",
      "model #00, epoch #049:   train_acc=100.0%   train_loss=7.87e-06   test_acc=018.1%   test_loss=4.42e+00\n",
      "model #01, epoch #000:   train_acc=007.5%   train_loss=1.70e-02   test_acc=009.5%   test_loss=4.06e+00\n",
      "model #01, epoch #001:   train_acc=031.2%   train_loss=1.29e-02   test_acc=013.3%   test_loss=3.84e+00\n",
      "model #01, epoch #002:   train_acc=055.7%   train_loss=9.83e-03   test_acc=014.5%   test_loss=3.75e+00\n",
      "model #01, epoch #003:   train_acc=073.8%   train_loss=7.14e-03   test_acc=015.6%   test_loss=3.73e+00\n",
      "model #01, epoch #004:   train_acc=085.9%   train_loss=5.09e-03   test_acc=014.6%   test_loss=3.77e+00\n",
      "model #01, epoch #005:   train_acc=091.4%   train_loss=3.81e-03   test_acc=015.1%   test_loss=3.82e+00\n",
      "model #01, epoch #006:   train_acc=094.2%   train_loss=2.93e-03   test_acc=015.4%   test_loss=3.88e+00\n",
      "model #01, epoch #007:   train_acc=096.8%   train_loss=2.11e-03   test_acc=015.9%   test_loss=3.93e+00\n",
      "model #01, epoch #008:   train_acc=098.6%   train_loss=1.36e-03   test_acc=015.4%   test_loss=3.96e+00\n",
      "model #01, epoch #009:   train_acc=099.7%   train_loss=6.98e-04   test_acc=016.0%   test_loss=4.01e+00\n",
      "model #01, epoch #010:   train_acc=099.9%   train_loss=3.93e-04   test_acc=015.8%   test_loss=4.03e+00\n",
      "model #01, epoch #011:   train_acc=100.0%   train_loss=2.33e-04   test_acc=016.1%   test_loss=4.06e+00\n",
      "model #01, epoch #012:   train_acc=100.0%   train_loss=1.54e-04   test_acc=016.2%   test_loss=4.08e+00\n",
      "model #01, epoch #013:   train_acc=100.0%   train_loss=1.16e-04   test_acc=016.4%   test_loss=4.10e+00\n",
      "model #01, epoch #014:   train_acc=100.0%   train_loss=9.06e-05   test_acc=016.2%   test_loss=4.12e+00\n",
      "model #01, epoch #015:   train_acc=100.0%   train_loss=7.30e-05   test_acc=016.3%   test_loss=4.13e+00\n",
      "model #01, epoch #016:   train_acc=100.0%   train_loss=6.12e-05   test_acc=016.2%   test_loss=4.14e+00\n",
      "model #01, epoch #017:   train_acc=100.0%   train_loss=5.26e-05   test_acc=016.3%   test_loss=4.15e+00\n",
      "model #01, epoch #018:   train_acc=100.0%   train_loss=4.61e-05   test_acc=016.2%   test_loss=4.17e+00\n",
      "model #01, epoch #019:   train_acc=100.0%   train_loss=4.08e-05   test_acc=016.1%   test_loss=4.18e+00\n",
      "model #01, epoch #020:   train_acc=100.0%   train_loss=3.64e-05   test_acc=016.1%   test_loss=4.19e+00\n",
      "model #01, epoch #021:   train_acc=100.0%   train_loss=3.28e-05   test_acc=016.2%   test_loss=4.20e+00\n",
      "model #01, epoch #022:   train_acc=100.0%   train_loss=2.98e-05   test_acc=016.0%   test_loss=4.21e+00\n",
      "model #01, epoch #023:   train_acc=100.0%   train_loss=2.71e-05   test_acc=016.1%   test_loss=4.22e+00\n",
      "model #01, epoch #024:   train_acc=100.0%   train_loss=2.49e-05   test_acc=016.0%   test_loss=4.23e+00\n",
      "model #01, epoch #025:   train_acc=100.0%   train_loss=2.29e-05   test_acc=016.0%   test_loss=4.23e+00\n",
      "model #01, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=016.0%   test_loss=4.24e+00\n",
      "model #01, epoch #027:   train_acc=100.0%   train_loss=1.96e-05   test_acc=015.9%   test_loss=4.25e+00\n",
      "model #01, epoch #028:   train_acc=100.0%   train_loss=1.83e-05   test_acc=015.9%   test_loss=4.26e+00\n",
      "model #01, epoch #029:   train_acc=100.0%   train_loss=1.71e-05   test_acc=015.9%   test_loss=4.27e+00\n",
      "model #01, epoch #030:   train_acc=100.0%   train_loss=1.60e-05   test_acc=015.9%   test_loss=4.28e+00\n",
      "model #01, epoch #031:   train_acc=100.0%   train_loss=1.50e-05   test_acc=015.8%   test_loss=4.28e+00\n",
      "model #01, epoch #032:   train_acc=100.0%   train_loss=1.41e-05   test_acc=015.8%   test_loss=4.29e+00\n",
      "model #01, epoch #033:   train_acc=100.0%   train_loss=1.32e-05   test_acc=015.9%   test_loss=4.30e+00\n",
      "model #01, epoch #034:   train_acc=100.0%   train_loss=1.25e-05   test_acc=015.9%   test_loss=4.30e+00\n",
      "model #01, epoch #035:   train_acc=100.0%   train_loss=1.18e-05   test_acc=015.8%   test_loss=4.31e+00\n",
      "model #01, epoch #036:   train_acc=100.0%   train_loss=1.11e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #01, epoch #037:   train_acc=100.0%   train_loss=1.05e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #01, epoch #038:   train_acc=100.0%   train_loss=9.99e-06   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #01, epoch #039:   train_acc=100.0%   train_loss=9.48e-06   test_acc=015.9%   test_loss=4.34e+00\n",
      "model #01, epoch #040:   train_acc=100.0%   train_loss=9.01e-06   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #01, epoch #041:   train_acc=100.0%   train_loss=8.57e-06   test_acc=016.0%   test_loss=4.35e+00\n",
      "model #01, epoch #042:   train_acc=100.0%   train_loss=8.16e-06   test_acc=015.9%   test_loss=4.36e+00\n",
      "model #01, epoch #043:   train_acc=100.0%   train_loss=7.77e-06   test_acc=016.1%   test_loss=4.36e+00\n",
      "model #01, epoch #044:   train_acc=100.0%   train_loss=7.41e-06   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #01, epoch #045:   train_acc=100.0%   train_loss=7.08e-06   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #01, epoch #046:   train_acc=100.0%   train_loss=6.76e-06   test_acc=016.0%   test_loss=4.38e+00\n",
      "model #01, epoch #047:   train_acc=100.0%   train_loss=6.46e-06   test_acc=016.0%   test_loss=4.39e+00\n",
      "model #01, epoch #048:   train_acc=100.0%   train_loss=6.18e-06   test_acc=016.0%   test_loss=4.39e+00\n",
      "model #01, epoch #049:   train_acc=100.0%   train_loss=5.91e-06   test_acc=016.0%   test_loss=4.40e+00\n",
      "model #02, epoch #000:   train_acc=009.1%   train_loss=1.67e-02   test_acc=009.9%   test_loss=4.03e+00\n",
      "model #02, epoch #001:   train_acc=029.9%   train_loss=1.29e-02   test_acc=012.7%   test_loss=3.84e+00\n",
      "model #02, epoch #002:   train_acc=052.2%   train_loss=9.92e-03   test_acc=014.4%   test_loss=3.77e+00\n",
      "model #02, epoch #003:   train_acc=071.3%   train_loss=7.27e-03   test_acc=014.6%   test_loss=3.77e+00\n",
      "model #02, epoch #004:   train_acc=084.5%   train_loss=5.18e-03   test_acc=014.4%   test_loss=3.80e+00\n",
      "model #02, epoch #005:   train_acc=091.0%   train_loss=3.83e-03   test_acc=014.5%   test_loss=3.85e+00\n",
      "model #02, epoch #006:   train_acc=095.2%   train_loss=2.71e-03   test_acc=014.0%   test_loss=3.91e+00\n",
      "model #02, epoch #007:   train_acc=098.0%   train_loss=1.78e-03   test_acc=014.7%   test_loss=3.97e+00\n",
      "model #02, epoch #008:   train_acc=099.3%   train_loss=1.17e-03   test_acc=014.9%   test_loss=4.02e+00\n",
      "model #02, epoch #009:   train_acc=099.8%   train_loss=7.58e-04   test_acc=015.2%   test_loss=4.06e+00\n",
      "model #02, epoch #010:   train_acc=100.0%   train_loss=4.54e-04   test_acc=015.6%   test_loss=4.06e+00\n",
      "model #02, epoch #011:   train_acc=100.0%   train_loss=2.67e-04   test_acc=015.2%   test_loss=4.09e+00\n",
      "model #02, epoch #012:   train_acc=100.0%   train_loss=1.67e-04   test_acc=015.3%   test_loss=4.13e+00\n",
      "model #02, epoch #013:   train_acc=100.0%   train_loss=1.16e-04   test_acc=015.7%   test_loss=4.15e+00\n",
      "model #02, epoch #014:   train_acc=100.0%   train_loss=8.99e-05   test_acc=015.7%   test_loss=4.17e+00\n",
      "model #02, epoch #015:   train_acc=100.0%   train_loss=7.29e-05   test_acc=016.0%   test_loss=4.19e+00\n",
      "model #02, epoch #016:   train_acc=100.0%   train_loss=6.09e-05   test_acc=016.0%   test_loss=4.20e+00\n",
      "model #02, epoch #017:   train_acc=100.0%   train_loss=5.21e-05   test_acc=015.9%   test_loss=4.21e+00\n",
      "model #02, epoch #018:   train_acc=100.0%   train_loss=4.53e-05   test_acc=015.7%   test_loss=4.23e+00\n",
      "model #02, epoch #019:   train_acc=100.0%   train_loss=3.99e-05   test_acc=015.7%   test_loss=4.24e+00\n",
      "model #02, epoch #020:   train_acc=100.0%   train_loss=3.57e-05   test_acc=015.8%   test_loss=4.25e+00\n",
      "model #02, epoch #021:   train_acc=100.0%   train_loss=3.21e-05   test_acc=015.9%   test_loss=4.26e+00\n",
      "model #02, epoch #022:   train_acc=100.0%   train_loss=2.91e-05   test_acc=015.8%   test_loss=4.27e+00\n",
      "model #02, epoch #023:   train_acc=100.0%   train_loss=2.66e-05   test_acc=015.9%   test_loss=4.28e+00\n",
      "model #02, epoch #024:   train_acc=100.0%   train_loss=2.43e-05   test_acc=015.9%   test_loss=4.29e+00\n",
      "model #02, epoch #025:   train_acc=100.0%   train_loss=2.24e-05   test_acc=015.8%   test_loss=4.30e+00\n",
      "model #02, epoch #026:   train_acc=100.0%   train_loss=2.07e-05   test_acc=015.9%   test_loss=4.30e+00\n",
      "model #02, epoch #027:   train_acc=100.0%   train_loss=1.92e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #02, epoch #028:   train_acc=100.0%   train_loss=1.79e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #02, epoch #029:   train_acc=100.0%   train_loss=1.67e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #02, epoch #030:   train_acc=100.0%   train_loss=1.56e-05   test_acc=015.9%   test_loss=4.34e+00\n",
      "model #02, epoch #031:   train_acc=100.0%   train_loss=1.46e-05   test_acc=015.9%   test_loss=4.35e+00\n",
      "model #02, epoch #032:   train_acc=100.0%   train_loss=1.37e-05   test_acc=015.8%   test_loss=4.35e+00\n",
      "model #02, epoch #033:   train_acc=100.0%   train_loss=1.29e-05   test_acc=015.8%   test_loss=4.36e+00\n",
      "model #02, epoch #034:   train_acc=100.0%   train_loss=1.21e-05   test_acc=015.7%   test_loss=4.37e+00\n",
      "model #02, epoch #035:   train_acc=100.0%   train_loss=1.14e-05   test_acc=015.7%   test_loss=4.38e+00\n",
      "model #02, epoch #036:   train_acc=100.0%   train_loss=1.08e-05   test_acc=015.8%   test_loss=4.38e+00\n",
      "model #02, epoch #037:   train_acc=100.0%   train_loss=1.02e-05   test_acc=015.7%   test_loss=4.39e+00\n",
      "model #02, epoch #038:   train_acc=100.0%   train_loss=9.69e-06   test_acc=015.7%   test_loss=4.40e+00\n",
      "model #02, epoch #039:   train_acc=100.0%   train_loss=9.19e-06   test_acc=015.7%   test_loss=4.40e+00\n",
      "model #02, epoch #040:   train_acc=100.0%   train_loss=8.73e-06   test_acc=015.6%   test_loss=4.41e+00\n",
      "model #02, epoch #041:   train_acc=100.0%   train_loss=8.30e-06   test_acc=015.7%   test_loss=4.42e+00\n",
      "model #02, epoch #042:   train_acc=100.0%   train_loss=7.90e-06   test_acc=015.7%   test_loss=4.42e+00\n",
      "model #02, epoch #043:   train_acc=100.0%   train_loss=7.52e-06   test_acc=015.6%   test_loss=4.43e+00\n",
      "model #02, epoch #044:   train_acc=100.0%   train_loss=7.17e-06   test_acc=015.6%   test_loss=4.44e+00\n",
      "model #02, epoch #045:   train_acc=100.0%   train_loss=6.84e-06   test_acc=015.6%   test_loss=4.44e+00\n",
      "model #02, epoch #046:   train_acc=100.0%   train_loss=6.54e-06   test_acc=015.5%   test_loss=4.45e+00\n",
      "model #02, epoch #047:   train_acc=100.0%   train_loss=6.25e-06   test_acc=015.5%   test_loss=4.46e+00\n",
      "model #02, epoch #048:   train_acc=100.0%   train_loss=5.97e-06   test_acc=015.6%   test_loss=4.46e+00\n",
      "model #02, epoch #049:   train_acc=100.0%   train_loss=5.72e-06   test_acc=015.6%   test_loss=4.47e+00\n",
      "model #03, epoch #000:   train_acc=008.5%   train_loss=1.69e-02   test_acc=010.5%   test_loss=4.06e+00\n",
      "model #03, epoch #001:   train_acc=030.2%   train_loss=1.30e-02   test_acc=013.9%   test_loss=3.85e+00\n",
      "model #03, epoch #002:   train_acc=052.9%   train_loss=9.98e-03   test_acc=014.7%   test_loss=3.78e+00\n",
      "model #03, epoch #003:   train_acc=071.8%   train_loss=7.30e-03   test_acc=015.6%   test_loss=3.76e+00\n",
      "model #03, epoch #004:   train_acc=083.7%   train_loss=5.27e-03   test_acc=014.8%   test_loss=3.79e+00\n",
      "model #03, epoch #005:   train_acc=089.5%   train_loss=4.01e-03   test_acc=015.3%   test_loss=3.81e+00\n",
      "model #03, epoch #006:   train_acc=093.6%   train_loss=2.96e-03   test_acc=015.1%   test_loss=3.87e+00\n",
      "model #03, epoch #007:   train_acc=097.0%   train_loss=1.98e-03   test_acc=014.9%   test_loss=3.93e+00\n",
      "model #03, epoch #008:   train_acc=099.1%   train_loss=1.19e-03   test_acc=016.4%   test_loss=3.96e+00\n",
      "model #03, epoch #009:   train_acc=099.8%   train_loss=6.82e-04   test_acc=016.0%   test_loss=4.01e+00\n",
      "model #03, epoch #010:   train_acc=099.9%   train_loss=4.08e-04   test_acc=016.0%   test_loss=4.03e+00\n",
      "model #03, epoch #011:   train_acc=100.0%   train_loss=2.54e-04   test_acc=016.4%   test_loss=4.07e+00\n",
      "model #03, epoch #012:   train_acc=100.0%   train_loss=1.69e-04   test_acc=016.4%   test_loss=4.10e+00\n",
      "model #03, epoch #013:   train_acc=100.0%   train_loss=1.27e-04   test_acc=015.8%   test_loss=4.12e+00\n",
      "model #03, epoch #014:   train_acc=100.0%   train_loss=9.94e-05   test_acc=016.0%   test_loss=4.14e+00\n",
      "model #03, epoch #015:   train_acc=100.0%   train_loss=7.93e-05   test_acc=016.2%   test_loss=4.16e+00\n",
      "model #03, epoch #016:   train_acc=100.0%   train_loss=6.57e-05   test_acc=016.4%   test_loss=4.17e+00\n",
      "model #03, epoch #017:   train_acc=100.0%   train_loss=5.57e-05   test_acc=016.4%   test_loss=4.19e+00\n",
      "model #03, epoch #018:   train_acc=100.0%   train_loss=4.81e-05   test_acc=016.4%   test_loss=4.20e+00\n",
      "model #03, epoch #019:   train_acc=100.0%   train_loss=4.21e-05   test_acc=016.2%   test_loss=4.21e+00\n",
      "model #03, epoch #020:   train_acc=100.0%   train_loss=3.73e-05   test_acc=016.2%   test_loss=4.22e+00\n",
      "model #03, epoch #021:   train_acc=100.0%   train_loss=3.34e-05   test_acc=016.4%   test_loss=4.23e+00\n",
      "model #03, epoch #022:   train_acc=100.0%   train_loss=3.01e-05   test_acc=016.3%   test_loss=4.24e+00\n",
      "model #03, epoch #023:   train_acc=100.0%   train_loss=2.74e-05   test_acc=016.4%   test_loss=4.25e+00\n",
      "model #03, epoch #024:   train_acc=100.0%   train_loss=2.50e-05   test_acc=016.6%   test_loss=4.26e+00\n",
      "model #03, epoch #025:   train_acc=100.0%   train_loss=2.30e-05   test_acc=016.5%   test_loss=4.27e+00\n",
      "model #03, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=016.3%   test_loss=4.27e+00\n",
      "model #03, epoch #027:   train_acc=100.0%   train_loss=1.96e-05   test_acc=016.3%   test_loss=4.28e+00\n",
      "model #03, epoch #028:   train_acc=100.0%   train_loss=1.83e-05   test_acc=016.2%   test_loss=4.29e+00\n",
      "model #03, epoch #029:   train_acc=100.0%   train_loss=1.70e-05   test_acc=016.3%   test_loss=4.30e+00\n",
      "model #03, epoch #030:   train_acc=100.0%   train_loss=1.59e-05   test_acc=016.2%   test_loss=4.31e+00\n",
      "model #03, epoch #031:   train_acc=100.0%   train_loss=1.49e-05   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #03, epoch #032:   train_acc=100.0%   train_loss=1.40e-05   test_acc=016.1%   test_loss=4.32e+00\n",
      "model #03, epoch #033:   train_acc=100.0%   train_loss=1.32e-05   test_acc=016.0%   test_loss=4.33e+00\n",
      "model #03, epoch #034:   train_acc=100.0%   train_loss=1.24e-05   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #03, epoch #035:   train_acc=100.0%   train_loss=1.17e-05   test_acc=016.1%   test_loss=4.35e+00\n",
      "model #03, epoch #036:   train_acc=100.0%   train_loss=1.11e-05   test_acc=016.1%   test_loss=4.35e+00\n",
      "model #03, epoch #037:   train_acc=100.0%   train_loss=1.05e-05   test_acc=016.0%   test_loss=4.36e+00\n",
      "model #03, epoch #038:   train_acc=100.0%   train_loss=9.94e-06   test_acc=015.9%   test_loss=4.37e+00\n",
      "model #03, epoch #039:   train_acc=100.0%   train_loss=9.43e-06   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #03, epoch #040:   train_acc=100.0%   train_loss=8.96e-06   test_acc=016.0%   test_loss=4.38e+00\n",
      "model #03, epoch #041:   train_acc=100.0%   train_loss=8.52e-06   test_acc=016.0%   test_loss=4.39e+00\n",
      "model #03, epoch #042:   train_acc=100.0%   train_loss=8.11e-06   test_acc=016.0%   test_loss=4.39e+00\n",
      "model #03, epoch #043:   train_acc=100.0%   train_loss=7.73e-06   test_acc=016.0%   test_loss=4.40e+00\n",
      "model #03, epoch #044:   train_acc=100.0%   train_loss=7.37e-06   test_acc=016.0%   test_loss=4.41e+00\n",
      "model #03, epoch #045:   train_acc=100.0%   train_loss=7.04e-06   test_acc=016.0%   test_loss=4.41e+00\n",
      "model #03, epoch #046:   train_acc=100.0%   train_loss=6.72e-06   test_acc=016.0%   test_loss=4.42e+00\n",
      "model #03, epoch #047:   train_acc=100.0%   train_loss=6.42e-06   test_acc=016.0%   test_loss=4.42e+00\n",
      "model #03, epoch #048:   train_acc=100.0%   train_loss=6.15e-06   test_acc=016.0%   test_loss=4.43e+00\n",
      "model #03, epoch #049:   train_acc=100.0%   train_loss=5.88e-06   test_acc=016.0%   test_loss=4.44e+00\n",
      "model #04, epoch #000:   train_acc=008.1%   train_loss=1.69e-02   test_acc=010.7%   test_loss=4.06e+00\n",
      "model #04, epoch #001:   train_acc=032.3%   train_loss=1.28e-02   test_acc=013.7%   test_loss=3.85e+00\n",
      "model #04, epoch #002:   train_acc=056.6%   train_loss=9.67e-03   test_acc=014.9%   test_loss=3.78e+00\n",
      "model #04, epoch #003:   train_acc=074.6%   train_loss=6.94e-03   test_acc=015.5%   test_loss=3.76e+00\n",
      "model #04, epoch #004:   train_acc=086.3%   train_loss=4.86e-03   test_acc=015.7%   test_loss=3.79e+00\n",
      "model #04, epoch #005:   train_acc=092.0%   train_loss=3.54e-03   test_acc=015.8%   test_loss=3.83e+00\n",
      "model #04, epoch #006:   train_acc=095.6%   train_loss=2.57e-03   test_acc=016.1%   test_loss=3.86e+00\n",
      "model #04, epoch #007:   train_acc=098.6%   train_loss=1.63e-03   test_acc=016.0%   test_loss=3.91e+00\n",
      "model #04, epoch #008:   train_acc=099.6%   train_loss=1.03e-03   test_acc=015.9%   test_loss=3.95e+00\n",
      "model #04, epoch #009:   train_acc=099.9%   train_loss=6.35e-04   test_acc=016.3%   test_loss=3.98e+00\n",
      "model #04, epoch #010:   train_acc=100.0%   train_loss=3.96e-04   test_acc=016.1%   test_loss=4.02e+00\n",
      "model #04, epoch #011:   train_acc=100.0%   train_loss=2.38e-04   test_acc=015.8%   test_loss=4.06e+00\n",
      "model #04, epoch #012:   train_acc=100.0%   train_loss=1.51e-04   test_acc=015.8%   test_loss=4.08e+00\n",
      "model #04, epoch #013:   train_acc=100.0%   train_loss=1.06e-04   test_acc=015.7%   test_loss=4.10e+00\n",
      "model #04, epoch #014:   train_acc=100.0%   train_loss=8.28e-05   test_acc=015.6%   test_loss=4.11e+00\n",
      "model #04, epoch #015:   train_acc=100.0%   train_loss=6.79e-05   test_acc=015.6%   test_loss=4.12e+00\n",
      "model #04, epoch #016:   train_acc=100.0%   train_loss=5.73e-05   test_acc=015.7%   test_loss=4.14e+00\n",
      "model #04, epoch #017:   train_acc=100.0%   train_loss=4.94e-05   test_acc=015.8%   test_loss=4.15e+00\n",
      "model #04, epoch #018:   train_acc=100.0%   train_loss=4.33e-05   test_acc=015.8%   test_loss=4.16e+00\n",
      "model #04, epoch #019:   train_acc=100.0%   train_loss=3.85e-05   test_acc=015.8%   test_loss=4.17e+00\n",
      "model #04, epoch #020:   train_acc=100.0%   train_loss=3.45e-05   test_acc=015.8%   test_loss=4.18e+00\n",
      "model #04, epoch #021:   train_acc=100.0%   train_loss=3.12e-05   test_acc=015.8%   test_loss=4.19e+00\n",
      "model #04, epoch #022:   train_acc=100.0%   train_loss=2.83e-05   test_acc=015.8%   test_loss=4.20e+00\n",
      "model #04, epoch #023:   train_acc=100.0%   train_loss=2.59e-05   test_acc=015.8%   test_loss=4.21e+00\n",
      "model #04, epoch #024:   train_acc=100.0%   train_loss=2.38e-05   test_acc=015.8%   test_loss=4.21e+00\n",
      "model #04, epoch #025:   train_acc=100.0%   train_loss=2.20e-05   test_acc=015.9%   test_loss=4.22e+00\n",
      "model #04, epoch #026:   train_acc=100.0%   train_loss=2.03e-05   test_acc=016.0%   test_loss=4.23e+00\n",
      "model #04, epoch #027:   train_acc=100.0%   train_loss=1.89e-05   test_acc=015.9%   test_loss=4.24e+00\n",
      "model #04, epoch #028:   train_acc=100.0%   train_loss=1.76e-05   test_acc=015.9%   test_loss=4.25e+00\n",
      "model #04, epoch #029:   train_acc=100.0%   train_loss=1.64e-05   test_acc=015.9%   test_loss=4.25e+00\n",
      "model #04, epoch #030:   train_acc=100.0%   train_loss=1.54e-05   test_acc=015.9%   test_loss=4.26e+00\n",
      "model #04, epoch #031:   train_acc=100.0%   train_loss=1.44e-05   test_acc=015.9%   test_loss=4.27e+00\n",
      "model #04, epoch #032:   train_acc=100.0%   train_loss=1.35e-05   test_acc=016.0%   test_loss=4.28e+00\n",
      "model #04, epoch #033:   train_acc=100.0%   train_loss=1.27e-05   test_acc=016.0%   test_loss=4.28e+00\n",
      "model #04, epoch #034:   train_acc=100.0%   train_loss=1.20e-05   test_acc=016.1%   test_loss=4.29e+00\n",
      "model #04, epoch #035:   train_acc=100.0%   train_loss=1.13e-05   test_acc=016.1%   test_loss=4.30e+00\n",
      "model #04, epoch #036:   train_acc=100.0%   train_loss=1.07e-05   test_acc=016.1%   test_loss=4.30e+00\n",
      "model #04, epoch #037:   train_acc=100.0%   train_loss=1.01e-05   test_acc=016.1%   test_loss=4.31e+00\n",
      "model #04, epoch #038:   train_acc=100.0%   train_loss=9.61e-06   test_acc=016.1%   test_loss=4.32e+00\n",
      "model #04, epoch #039:   train_acc=100.0%   train_loss=9.12e-06   test_acc=016.0%   test_loss=4.32e+00\n",
      "model #04, epoch #040:   train_acc=100.0%   train_loss=8.66e-06   test_acc=016.0%   test_loss=4.33e+00\n",
      "model #04, epoch #041:   train_acc=100.0%   train_loss=8.24e-06   test_acc=016.0%   test_loss=4.33e+00\n",
      "model #04, epoch #042:   train_acc=100.0%   train_loss=7.84e-06   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #04, epoch #043:   train_acc=100.0%   train_loss=7.47e-06   test_acc=016.0%   test_loss=4.35e+00\n",
      "model #04, epoch #044:   train_acc=100.0%   train_loss=7.12e-06   test_acc=016.1%   test_loss=4.35e+00\n",
      "model #04, epoch #045:   train_acc=100.0%   train_loss=6.80e-06   test_acc=016.1%   test_loss=4.36e+00\n",
      "model #04, epoch #046:   train_acc=100.0%   train_loss=6.49e-06   test_acc=016.2%   test_loss=4.36e+00\n",
      "model #04, epoch #047:   train_acc=100.0%   train_loss=6.20e-06   test_acc=016.2%   test_loss=4.37e+00\n",
      "model #04, epoch #048:   train_acc=100.0%   train_loss=5.93e-06   test_acc=016.2%   test_loss=4.37e+00\n",
      "model #04, epoch #049:   train_acc=100.0%   train_loss=5.68e-06   test_acc=016.2%   test_loss=4.38e+00\n",
      "model #05, epoch #000:   train_acc=008.1%   train_loss=1.70e-02   test_acc=009.9%   test_loss=4.08e+00\n",
      "model #05, epoch #001:   train_acc=030.6%   train_loss=1.30e-02   test_acc=012.3%   test_loss=3.87e+00\n",
      "model #05, epoch #002:   train_acc=054.8%   train_loss=9.89e-03   test_acc=013.7%   test_loss=3.80e+00\n",
      "model #05, epoch #003:   train_acc=074.1%   train_loss=7.19e-03   test_acc=014.1%   test_loss=3.79e+00\n",
      "model #05, epoch #004:   train_acc=085.9%   train_loss=5.11e-03   test_acc=014.7%   test_loss=3.81e+00\n",
      "model #05, epoch #005:   train_acc=091.1%   train_loss=3.83e-03   test_acc=014.5%   test_loss=3.85e+00\n",
      "model #05, epoch #006:   train_acc=095.3%   train_loss=2.70e-03   test_acc=013.4%   test_loss=3.90e+00\n",
      "model #05, epoch #007:   train_acc=098.0%   train_loss=1.79e-03   test_acc=014.2%   test_loss=3.97e+00\n",
      "model #05, epoch #008:   train_acc=099.3%   train_loss=1.19e-03   test_acc=014.1%   test_loss=4.03e+00\n",
      "model #05, epoch #009:   train_acc=099.9%   train_loss=7.16e-04   test_acc=015.7%   test_loss=4.06e+00\n",
      "model #05, epoch #010:   train_acc=100.0%   train_loss=4.14e-04   test_acc=014.8%   test_loss=4.07e+00\n",
      "model #05, epoch #011:   train_acc=100.0%   train_loss=2.48e-04   test_acc=014.7%   test_loss=4.10e+00\n",
      "model #05, epoch #012:   train_acc=100.0%   train_loss=1.65e-04   test_acc=015.2%   test_loss=4.12e+00\n",
      "model #05, epoch #013:   train_acc=100.0%   train_loss=1.17e-04   test_acc=015.0%   test_loss=4.14e+00\n",
      "model #05, epoch #014:   train_acc=100.0%   train_loss=8.95e-05   test_acc=015.2%   test_loss=4.16e+00\n",
      "model #05, epoch #015:   train_acc=100.0%   train_loss=7.25e-05   test_acc=015.4%   test_loss=4.17e+00\n",
      "model #05, epoch #016:   train_acc=100.0%   train_loss=6.06e-05   test_acc=015.4%   test_loss=4.19e+00\n",
      "model #05, epoch #017:   train_acc=100.0%   train_loss=5.20e-05   test_acc=015.3%   test_loss=4.20e+00\n",
      "model #05, epoch #018:   train_acc=100.0%   train_loss=4.52e-05   test_acc=015.3%   test_loss=4.21e+00\n",
      "model #05, epoch #019:   train_acc=100.0%   train_loss=3.99e-05   test_acc=015.3%   test_loss=4.22e+00\n",
      "model #05, epoch #020:   train_acc=100.0%   train_loss=3.55e-05   test_acc=015.3%   test_loss=4.23e+00\n",
      "model #05, epoch #021:   train_acc=100.0%   train_loss=3.20e-05   test_acc=015.1%   test_loss=4.24e+00\n",
      "model #05, epoch #022:   train_acc=100.0%   train_loss=2.90e-05   test_acc=015.2%   test_loss=4.25e+00\n",
      "model #05, epoch #023:   train_acc=100.0%   train_loss=2.64e-05   test_acc=015.2%   test_loss=4.26e+00\n",
      "model #05, epoch #024:   train_acc=100.0%   train_loss=2.42e-05   test_acc=015.2%   test_loss=4.27e+00\n",
      "model #05, epoch #025:   train_acc=100.0%   train_loss=2.23e-05   test_acc=015.3%   test_loss=4.28e+00\n",
      "model #05, epoch #026:   train_acc=100.0%   train_loss=2.06e-05   test_acc=015.1%   test_loss=4.29e+00\n",
      "model #05, epoch #027:   train_acc=100.0%   train_loss=1.91e-05   test_acc=015.1%   test_loss=4.29e+00\n",
      "model #05, epoch #028:   train_acc=100.0%   train_loss=1.77e-05   test_acc=015.2%   test_loss=4.30e+00\n",
      "model #05, epoch #029:   train_acc=100.0%   train_loss=1.65e-05   test_acc=015.2%   test_loss=4.31e+00\n",
      "model #05, epoch #030:   train_acc=100.0%   train_loss=1.55e-05   test_acc=015.2%   test_loss=4.32e+00\n",
      "model #05, epoch #031:   train_acc=100.0%   train_loss=1.45e-05   test_acc=015.2%   test_loss=4.32e+00\n",
      "model #05, epoch #032:   train_acc=100.0%   train_loss=1.36e-05   test_acc=015.1%   test_loss=4.33e+00\n",
      "model #05, epoch #033:   train_acc=100.0%   train_loss=1.28e-05   test_acc=015.3%   test_loss=4.34e+00\n",
      "model #05, epoch #034:   train_acc=100.0%   train_loss=1.21e-05   test_acc=015.2%   test_loss=4.35e+00\n",
      "model #05, epoch #035:   train_acc=100.0%   train_loss=1.14e-05   test_acc=015.2%   test_loss=4.35e+00\n",
      "model #05, epoch #036:   train_acc=100.0%   train_loss=1.08e-05   test_acc=015.2%   test_loss=4.36e+00\n",
      "model #05, epoch #037:   train_acc=100.0%   train_loss=1.02e-05   test_acc=015.2%   test_loss=4.37e+00\n",
      "model #05, epoch #038:   train_acc=100.0%   train_loss=9.64e-06   test_acc=015.1%   test_loss=4.37e+00\n",
      "model #05, epoch #039:   train_acc=100.0%   train_loss=9.14e-06   test_acc=015.1%   test_loss=4.38e+00\n",
      "model #05, epoch #040:   train_acc=100.0%   train_loss=8.68e-06   test_acc=015.0%   test_loss=4.38e+00\n",
      "model #05, epoch #041:   train_acc=100.0%   train_loss=8.25e-06   test_acc=015.0%   test_loss=4.39e+00\n",
      "model #05, epoch #042:   train_acc=100.0%   train_loss=7.85e-06   test_acc=015.0%   test_loss=4.40e+00\n",
      "model #05, epoch #043:   train_acc=100.0%   train_loss=7.48e-06   test_acc=015.1%   test_loss=4.40e+00\n",
      "model #05, epoch #044:   train_acc=100.0%   train_loss=7.13e-06   test_acc=015.1%   test_loss=4.41e+00\n",
      "model #05, epoch #045:   train_acc=100.0%   train_loss=6.80e-06   test_acc=015.2%   test_loss=4.41e+00\n",
      "model #05, epoch #046:   train_acc=100.0%   train_loss=6.49e-06   test_acc=015.2%   test_loss=4.42e+00\n",
      "model #05, epoch #047:   train_acc=100.0%   train_loss=6.20e-06   test_acc=015.1%   test_loss=4.43e+00\n",
      "model #05, epoch #048:   train_acc=100.0%   train_loss=5.93e-06   test_acc=015.1%   test_loss=4.43e+00\n",
      "model #05, epoch #049:   train_acc=100.0%   train_loss=5.68e-06   test_acc=015.1%   test_loss=4.44e+00\n",
      "model #06, epoch #000:   train_acc=007.8%   train_loss=1.69e-02   test_acc=010.2%   test_loss=4.08e+00\n",
      "model #06, epoch #001:   train_acc=029.5%   train_loss=1.30e-02   test_acc=013.0%   test_loss=3.87e+00\n",
      "model #06, epoch #002:   train_acc=052.6%   train_loss=9.99e-03   test_acc=013.8%   test_loss=3.80e+00\n",
      "model #06, epoch #003:   train_acc=072.0%   train_loss=7.30e-03   test_acc=014.0%   test_loss=3.79e+00\n",
      "model #06, epoch #004:   train_acc=084.9%   train_loss=5.15e-03   test_acc=014.2%   test_loss=3.81e+00\n",
      "model #06, epoch #005:   train_acc=090.8%   train_loss=3.83e-03   test_acc=014.6%   test_loss=3.86e+00\n",
      "model #06, epoch #006:   train_acc=094.2%   train_loss=2.86e-03   test_acc=014.3%   test_loss=3.91e+00\n",
      "model #06, epoch #007:   train_acc=097.3%   train_loss=1.94e-03   test_acc=014.2%   test_loss=3.98e+00\n",
      "model #06, epoch #008:   train_acc=099.0%   train_loss=1.32e-03   test_acc=014.1%   test_loss=4.02e+00\n",
      "model #06, epoch #009:   train_acc=099.7%   train_loss=8.42e-04   test_acc=014.3%   test_loss=4.06e+00\n",
      "model #06, epoch #010:   train_acc=099.9%   train_loss=5.15e-04   test_acc=015.4%   test_loss=4.07e+00\n",
      "model #06, epoch #011:   train_acc=100.0%   train_loss=2.95e-04   test_acc=015.1%   test_loss=4.11e+00\n",
      "model #06, epoch #012:   train_acc=100.0%   train_loss=1.79e-04   test_acc=015.1%   test_loss=4.13e+00\n",
      "model #06, epoch #013:   train_acc=100.0%   train_loss=1.24e-04   test_acc=015.4%   test_loss=4.16e+00\n",
      "model #06, epoch #014:   train_acc=100.0%   train_loss=9.60e-05   test_acc=015.6%   test_loss=4.18e+00\n",
      "model #06, epoch #015:   train_acc=100.0%   train_loss=7.81e-05   test_acc=015.4%   test_loss=4.20e+00\n",
      "model #06, epoch #016:   train_acc=100.0%   train_loss=6.52e-05   test_acc=015.4%   test_loss=4.21e+00\n",
      "model #06, epoch #017:   train_acc=100.0%   train_loss=5.53e-05   test_acc=015.2%   test_loss=4.22e+00\n",
      "model #06, epoch #018:   train_acc=100.0%   train_loss=4.75e-05   test_acc=015.3%   test_loss=4.23e+00\n",
      "model #06, epoch #019:   train_acc=100.0%   train_loss=4.13e-05   test_acc=015.3%   test_loss=4.24e+00\n",
      "model #06, epoch #020:   train_acc=100.0%   train_loss=3.65e-05   test_acc=015.2%   test_loss=4.25e+00\n",
      "model #06, epoch #021:   train_acc=100.0%   train_loss=3.25e-05   test_acc=015.4%   test_loss=4.26e+00\n",
      "model #06, epoch #022:   train_acc=100.0%   train_loss=2.92e-05   test_acc=015.5%   test_loss=4.27e+00\n",
      "model #06, epoch #023:   train_acc=100.0%   train_loss=2.65e-05   test_acc=015.4%   test_loss=4.28e+00\n",
      "model #06, epoch #024:   train_acc=100.0%   train_loss=2.42e-05   test_acc=015.4%   test_loss=4.29e+00\n",
      "model #06, epoch #025:   train_acc=100.0%   train_loss=2.22e-05   test_acc=015.3%   test_loss=4.30e+00\n",
      "model #06, epoch #026:   train_acc=100.0%   train_loss=2.05e-05   test_acc=015.3%   test_loss=4.30e+00\n",
      "model #06, epoch #027:   train_acc=100.0%   train_loss=1.89e-05   test_acc=015.2%   test_loss=4.31e+00\n",
      "model #06, epoch #028:   train_acc=100.0%   train_loss=1.76e-05   test_acc=015.3%   test_loss=4.32e+00\n",
      "model #06, epoch #029:   train_acc=100.0%   train_loss=1.64e-05   test_acc=015.3%   test_loss=4.33e+00\n",
      "model #06, epoch #030:   train_acc=100.0%   train_loss=1.53e-05   test_acc=015.3%   test_loss=4.34e+00\n",
      "model #06, epoch #031:   train_acc=100.0%   train_loss=1.44e-05   test_acc=015.3%   test_loss=4.34e+00\n",
      "model #06, epoch #032:   train_acc=100.0%   train_loss=1.35e-05   test_acc=015.3%   test_loss=4.35e+00\n",
      "model #06, epoch #033:   train_acc=100.0%   train_loss=1.27e-05   test_acc=015.3%   test_loss=4.36e+00\n",
      "model #06, epoch #034:   train_acc=100.0%   train_loss=1.20e-05   test_acc=015.3%   test_loss=4.36e+00\n",
      "model #06, epoch #035:   train_acc=100.0%   train_loss=1.13e-05   test_acc=015.4%   test_loss=4.37e+00\n",
      "model #06, epoch #036:   train_acc=100.0%   train_loss=1.07e-05   test_acc=015.4%   test_loss=4.38e+00\n",
      "model #06, epoch #037:   train_acc=100.0%   train_loss=1.01e-05   test_acc=015.4%   test_loss=4.38e+00\n",
      "model #06, epoch #038:   train_acc=100.0%   train_loss=9.61e-06   test_acc=015.4%   test_loss=4.39e+00\n",
      "model #06, epoch #039:   train_acc=100.0%   train_loss=9.12e-06   test_acc=015.4%   test_loss=4.40e+00\n",
      "model #06, epoch #040:   train_acc=100.0%   train_loss=8.67e-06   test_acc=015.4%   test_loss=4.40e+00\n",
      "model #06, epoch #041:   train_acc=100.0%   train_loss=8.25e-06   test_acc=015.4%   test_loss=4.41e+00\n",
      "model #06, epoch #042:   train_acc=100.0%   train_loss=7.86e-06   test_acc=015.4%   test_loss=4.42e+00\n",
      "model #06, epoch #043:   train_acc=100.0%   train_loss=7.49e-06   test_acc=015.3%   test_loss=4.42e+00\n",
      "model #06, epoch #044:   train_acc=100.0%   train_loss=7.15e-06   test_acc=015.4%   test_loss=4.43e+00\n",
      "model #06, epoch #045:   train_acc=100.0%   train_loss=6.83e-06   test_acc=015.4%   test_loss=4.43e+00\n",
      "model #06, epoch #046:   train_acc=100.0%   train_loss=6.52e-06   test_acc=015.4%   test_loss=4.44e+00\n",
      "model #06, epoch #047:   train_acc=100.0%   train_loss=6.24e-06   test_acc=015.5%   test_loss=4.45e+00\n",
      "model #06, epoch #048:   train_acc=100.0%   train_loss=5.97e-06   test_acc=015.5%   test_loss=4.45e+00\n",
      "model #06, epoch #049:   train_acc=100.0%   train_loss=5.72e-06   test_acc=015.5%   test_loss=4.46e+00\n",
      "model #07, epoch #000:   train_acc=008.1%   train_loss=1.69e-02   test_acc=010.8%   test_loss=4.03e+00\n",
      "model #07, epoch #001:   train_acc=031.4%   train_loss=1.29e-02   test_acc=013.5%   test_loss=3.83e+00\n",
      "model #07, epoch #002:   train_acc=054.6%   train_loss=9.80e-03   test_acc=014.9%   test_loss=3.76e+00\n",
      "model #07, epoch #003:   train_acc=072.7%   train_loss=7.16e-03   test_acc=015.5%   test_loss=3.75e+00\n",
      "model #07, epoch #004:   train_acc=084.6%   train_loss=5.16e-03   test_acc=015.6%   test_loss=3.77e+00\n",
      "model #07, epoch #005:   train_acc=090.1%   train_loss=3.87e-03   test_acc=015.6%   test_loss=3.83e+00\n",
      "model #07, epoch #006:   train_acc=094.0%   train_loss=2.87e-03   test_acc=016.0%   test_loss=3.87e+00\n",
      "model #07, epoch #007:   train_acc=097.2%   train_loss=1.92e-03   test_acc=015.7%   test_loss=3.93e+00\n",
      "model #07, epoch #008:   train_acc=099.1%   train_loss=1.20e-03   test_acc=016.0%   test_loss=3.99e+00\n",
      "model #07, epoch #009:   train_acc=099.7%   train_loss=6.85e-04   test_acc=016.0%   test_loss=4.05e+00\n",
      "model #07, epoch #010:   train_acc=100.0%   train_loss=3.75e-04   test_acc=016.8%   test_loss=4.07e+00\n",
      "model #07, epoch #011:   train_acc=100.0%   train_loss=2.39e-04   test_acc=016.8%   test_loss=4.08e+00\n",
      "model #07, epoch #012:   train_acc=100.0%   train_loss=1.62e-04   test_acc=017.2%   test_loss=4.10e+00\n",
      "model #07, epoch #013:   train_acc=100.0%   train_loss=1.19e-04   test_acc=017.0%   test_loss=4.12e+00\n",
      "model #07, epoch #014:   train_acc=100.0%   train_loss=9.49e-05   test_acc=016.8%   test_loss=4.14e+00\n",
      "model #07, epoch #015:   train_acc=100.0%   train_loss=7.79e-05   test_acc=016.9%   test_loss=4.16e+00\n",
      "model #07, epoch #016:   train_acc=100.0%   train_loss=6.55e-05   test_acc=016.8%   test_loss=4.17e+00\n",
      "model #07, epoch #017:   train_acc=100.0%   train_loss=5.58e-05   test_acc=016.9%   test_loss=4.18e+00\n",
      "model #07, epoch #018:   train_acc=100.0%   train_loss=4.83e-05   test_acc=016.8%   test_loss=4.20e+00\n",
      "model #07, epoch #019:   train_acc=100.0%   train_loss=4.23e-05   test_acc=016.7%   test_loss=4.21e+00\n",
      "model #07, epoch #020:   train_acc=100.0%   train_loss=3.74e-05   test_acc=016.5%   test_loss=4.22e+00\n",
      "model #07, epoch #021:   train_acc=100.0%   train_loss=3.35e-05   test_acc=016.6%   test_loss=4.23e+00\n",
      "model #07, epoch #022:   train_acc=100.0%   train_loss=3.02e-05   test_acc=016.5%   test_loss=4.24e+00\n",
      "model #07, epoch #023:   train_acc=100.0%   train_loss=2.74e-05   test_acc=016.6%   test_loss=4.25e+00\n",
      "model #07, epoch #024:   train_acc=100.0%   train_loss=2.50e-05   test_acc=016.6%   test_loss=4.26e+00\n",
      "model #07, epoch #025:   train_acc=100.0%   train_loss=2.29e-05   test_acc=016.7%   test_loss=4.26e+00\n",
      "model #07, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=016.7%   test_loss=4.27e+00\n",
      "model #07, epoch #027:   train_acc=100.0%   train_loss=1.96e-05   test_acc=016.6%   test_loss=4.28e+00\n",
      "model #07, epoch #028:   train_acc=100.0%   train_loss=1.82e-05   test_acc=016.7%   test_loss=4.29e+00\n",
      "model #07, epoch #029:   train_acc=100.0%   train_loss=1.69e-05   test_acc=016.8%   test_loss=4.30e+00\n",
      "model #07, epoch #030:   train_acc=100.0%   train_loss=1.58e-05   test_acc=016.8%   test_loss=4.30e+00\n",
      "model #07, epoch #031:   train_acc=100.0%   train_loss=1.48e-05   test_acc=016.7%   test_loss=4.31e+00\n",
      "model #07, epoch #032:   train_acc=100.0%   train_loss=1.39e-05   test_acc=016.8%   test_loss=4.32e+00\n",
      "model #07, epoch #033:   train_acc=100.0%   train_loss=1.31e-05   test_acc=016.8%   test_loss=4.32e+00\n",
      "model #07, epoch #034:   train_acc=100.0%   train_loss=1.23e-05   test_acc=016.8%   test_loss=4.33e+00\n",
      "model #07, epoch #035:   train_acc=100.0%   train_loss=1.16e-05   test_acc=016.9%   test_loss=4.34e+00\n",
      "model #07, epoch #036:   train_acc=100.0%   train_loss=1.10e-05   test_acc=016.9%   test_loss=4.34e+00\n",
      "model #07, epoch #037:   train_acc=100.0%   train_loss=1.04e-05   test_acc=016.9%   test_loss=4.35e+00\n",
      "model #07, epoch #038:   train_acc=100.0%   train_loss=9.86e-06   test_acc=016.9%   test_loss=4.36e+00\n",
      "model #07, epoch #039:   train_acc=100.0%   train_loss=9.36e-06   test_acc=016.9%   test_loss=4.36e+00\n",
      "model #07, epoch #040:   train_acc=100.0%   train_loss=8.89e-06   test_acc=016.9%   test_loss=4.37e+00\n",
      "model #07, epoch #041:   train_acc=100.0%   train_loss=8.45e-06   test_acc=016.9%   test_loss=4.38e+00\n",
      "model #07, epoch #042:   train_acc=100.0%   train_loss=8.05e-06   test_acc=017.0%   test_loss=4.38e+00\n",
      "model #07, epoch #043:   train_acc=100.0%   train_loss=7.67e-06   test_acc=017.0%   test_loss=4.39e+00\n",
      "model #07, epoch #044:   train_acc=100.0%   train_loss=7.31e-06   test_acc=017.0%   test_loss=4.39e+00\n",
      "model #07, epoch #045:   train_acc=100.0%   train_loss=6.98e-06   test_acc=016.9%   test_loss=4.40e+00\n",
      "model #07, epoch #046:   train_acc=100.0%   train_loss=6.67e-06   test_acc=016.9%   test_loss=4.41e+00\n",
      "model #07, epoch #047:   train_acc=100.0%   train_loss=6.37e-06   test_acc=016.9%   test_loss=4.41e+00\n",
      "model #07, epoch #048:   train_acc=100.0%   train_loss=6.10e-06   test_acc=016.9%   test_loss=4.42e+00\n",
      "model #07, epoch #049:   train_acc=100.0%   train_loss=5.84e-06   test_acc=016.9%   test_loss=4.42e+00\n",
      "model #08, epoch #000:   train_acc=008.2%   train_loss=1.68e-02   test_acc=011.0%   test_loss=4.03e+00\n",
      "model #08, epoch #001:   train_acc=031.4%   train_loss=1.29e-02   test_acc=014.4%   test_loss=3.83e+00\n",
      "model #08, epoch #002:   train_acc=054.9%   train_loss=9.83e-03   test_acc=015.2%   test_loss=3.76e+00\n",
      "model #08, epoch #003:   train_acc=073.8%   train_loss=7.11e-03   test_acc=015.5%   test_loss=3.75e+00\n",
      "model #08, epoch #004:   train_acc=086.3%   train_loss=5.01e-03   test_acc=015.9%   test_loss=3.78e+00\n",
      "model #08, epoch #005:   train_acc=091.5%   train_loss=3.77e-03   test_acc=014.8%   test_loss=3.82e+00\n",
      "model #08, epoch #006:   train_acc=095.8%   train_loss=2.60e-03   test_acc=015.5%   test_loss=3.87e+00\n",
      "model #08, epoch #007:   train_acc=098.5%   train_loss=1.64e-03   test_acc=016.3%   test_loss=3.93e+00\n",
      "model #08, epoch #008:   train_acc=099.7%   train_loss=1.01e-03   test_acc=016.3%   test_loss=3.98e+00\n",
      "model #08, epoch #009:   train_acc=099.9%   train_loss=6.15e-04   test_acc=016.8%   test_loss=4.01e+00\n",
      "model #08, epoch #010:   train_acc=100.0%   train_loss=3.72e-04   test_acc=016.4%   test_loss=4.03e+00\n",
      "model #08, epoch #011:   train_acc=100.0%   train_loss=2.50e-04   test_acc=016.4%   test_loss=4.04e+00\n",
      "model #08, epoch #012:   train_acc=100.0%   train_loss=1.66e-04   test_acc=016.6%   test_loss=4.06e+00\n",
      "model #08, epoch #013:   train_acc=100.0%   train_loss=1.15e-04   test_acc=016.6%   test_loss=4.09e+00\n",
      "model #08, epoch #014:   train_acc=100.0%   train_loss=8.85e-05   test_acc=016.5%   test_loss=4.11e+00\n",
      "model #08, epoch #015:   train_acc=100.0%   train_loss=7.14e-05   test_acc=016.6%   test_loss=4.12e+00\n",
      "model #08, epoch #016:   train_acc=100.0%   train_loss=5.93e-05   test_acc=016.7%   test_loss=4.13e+00\n",
      "model #08, epoch #017:   train_acc=100.0%   train_loss=5.06e-05   test_acc=016.8%   test_loss=4.14e+00\n",
      "model #08, epoch #018:   train_acc=100.0%   train_loss=4.37e-05   test_acc=016.6%   test_loss=4.15e+00\n",
      "model #08, epoch #019:   train_acc=100.0%   train_loss=3.87e-05   test_acc=016.7%   test_loss=4.17e+00\n",
      "model #08, epoch #020:   train_acc=100.0%   train_loss=3.47e-05   test_acc=016.7%   test_loss=4.18e+00\n",
      "model #08, epoch #021:   train_acc=100.0%   train_loss=3.14e-05   test_acc=016.7%   test_loss=4.18e+00\n",
      "model #08, epoch #022:   train_acc=100.0%   train_loss=2.86e-05   test_acc=016.6%   test_loss=4.19e+00\n",
      "model #08, epoch #023:   train_acc=100.0%   train_loss=2.62e-05   test_acc=016.7%   test_loss=4.20e+00\n",
      "model #08, epoch #024:   train_acc=100.0%   train_loss=2.41e-05   test_acc=016.7%   test_loss=4.21e+00\n",
      "model #08, epoch #025:   train_acc=100.0%   train_loss=2.22e-05   test_acc=016.6%   test_loss=4.22e+00\n",
      "model #08, epoch #026:   train_acc=100.0%   train_loss=2.06e-05   test_acc=016.6%   test_loss=4.23e+00\n",
      "model #08, epoch #027:   train_acc=100.0%   train_loss=1.92e-05   test_acc=016.7%   test_loss=4.24e+00\n",
      "model #08, epoch #028:   train_acc=100.0%   train_loss=1.79e-05   test_acc=016.8%   test_loss=4.24e+00\n",
      "model #08, epoch #029:   train_acc=100.0%   train_loss=1.67e-05   test_acc=016.9%   test_loss=4.25e+00\n",
      "model #08, epoch #030:   train_acc=100.0%   train_loss=1.56e-05   test_acc=016.9%   test_loss=4.26e+00\n",
      "model #08, epoch #031:   train_acc=100.0%   train_loss=1.47e-05   test_acc=017.0%   test_loss=4.27e+00\n",
      "model #08, epoch #032:   train_acc=100.0%   train_loss=1.38e-05   test_acc=017.0%   test_loss=4.27e+00\n",
      "model #08, epoch #033:   train_acc=100.0%   train_loss=1.30e-05   test_acc=017.1%   test_loss=4.28e+00\n",
      "model #08, epoch #034:   train_acc=100.0%   train_loss=1.23e-05   test_acc=017.1%   test_loss=4.29e+00\n",
      "model #08, epoch #035:   train_acc=100.0%   train_loss=1.16e-05   test_acc=017.1%   test_loss=4.29e+00\n",
      "model #08, epoch #036:   train_acc=100.0%   train_loss=1.10e-05   test_acc=017.1%   test_loss=4.30e+00\n",
      "model #08, epoch #037:   train_acc=100.0%   train_loss=1.04e-05   test_acc=017.1%   test_loss=4.31e+00\n",
      "model #08, epoch #038:   train_acc=100.0%   train_loss=9.84e-06   test_acc=017.1%   test_loss=4.32e+00\n",
      "model #08, epoch #039:   train_acc=100.0%   train_loss=9.34e-06   test_acc=017.1%   test_loss=4.32e+00\n",
      "model #08, epoch #040:   train_acc=100.0%   train_loss=8.87e-06   test_acc=017.1%   test_loss=4.33e+00\n",
      "model #08, epoch #041:   train_acc=100.0%   train_loss=8.44e-06   test_acc=017.0%   test_loss=4.33e+00\n",
      "model #08, epoch #042:   train_acc=100.0%   train_loss=8.04e-06   test_acc=017.0%   test_loss=4.34e+00\n",
      "model #08, epoch #043:   train_acc=100.0%   train_loss=7.66e-06   test_acc=017.1%   test_loss=4.35e+00\n",
      "model #08, epoch #044:   train_acc=100.0%   train_loss=7.30e-06   test_acc=017.0%   test_loss=4.35e+00\n",
      "model #08, epoch #045:   train_acc=100.0%   train_loss=6.97e-06   test_acc=017.0%   test_loss=4.36e+00\n",
      "model #08, epoch #046:   train_acc=100.0%   train_loss=6.66e-06   test_acc=017.0%   test_loss=4.36e+00\n",
      "model #08, epoch #047:   train_acc=100.0%   train_loss=6.36e-06   test_acc=017.0%   test_loss=4.37e+00\n",
      "model #08, epoch #048:   train_acc=100.0%   train_loss=6.08e-06   test_acc=016.9%   test_loss=4.38e+00\n",
      "model #08, epoch #049:   train_acc=100.0%   train_loss=5.82e-06   test_acc=017.0%   test_loss=4.38e+00\n",
      "model #09, epoch #000:   train_acc=008.4%   train_loss=1.69e-02   test_acc=010.3%   test_loss=4.07e+00\n",
      "model #09, epoch #001:   train_acc=030.6%   train_loss=1.29e-02   test_acc=014.0%   test_loss=3.85e+00\n",
      "model #09, epoch #002:   train_acc=054.4%   train_loss=9.87e-03   test_acc=014.3%   test_loss=3.77e+00\n",
      "model #09, epoch #003:   train_acc=073.4%   train_loss=7.17e-03   test_acc=015.1%   test_loss=3.75e+00\n",
      "model #09, epoch #004:   train_acc=085.0%   train_loss=5.12e-03   test_acc=015.3%   test_loss=3.77e+00\n",
      "model #09, epoch #005:   train_acc=090.4%   train_loss=3.89e-03   test_acc=014.7%   test_loss=3.81e+00\n",
      "model #09, epoch #006:   train_acc=094.5%   train_loss=2.84e-03   test_acc=014.5%   test_loss=3.87e+00\n",
      "model #09, epoch #007:   train_acc=097.4%   train_loss=1.96e-03   test_acc=015.6%   test_loss=3.93e+00\n",
      "model #09, epoch #008:   train_acc=098.9%   train_loss=1.31e-03   test_acc=015.3%   test_loss=3.98e+00\n",
      "model #09, epoch #009:   train_acc=099.7%   train_loss=7.95e-04   test_acc=015.7%   test_loss=4.02e+00\n",
      "model #09, epoch #010:   train_acc=099.9%   train_loss=4.68e-04   test_acc=016.0%   test_loss=4.05e+00\n",
      "model #09, epoch #011:   train_acc=100.0%   train_loss=2.78e-04   test_acc=016.4%   test_loss=4.09e+00\n",
      "model #09, epoch #012:   train_acc=100.0%   train_loss=1.78e-04   test_acc=015.6%   test_loss=4.10e+00\n",
      "model #09, epoch #013:   train_acc=100.0%   train_loss=1.19e-04   test_acc=015.7%   test_loss=4.12e+00\n",
      "model #09, epoch #014:   train_acc=100.0%   train_loss=9.10e-05   test_acc=015.7%   test_loss=4.13e+00\n",
      "model #09, epoch #015:   train_acc=100.0%   train_loss=7.37e-05   test_acc=016.2%   test_loss=4.14e+00\n",
      "model #09, epoch #016:   train_acc=100.0%   train_loss=6.21e-05   test_acc=016.0%   test_loss=4.15e+00\n",
      "model #09, epoch #017:   train_acc=100.0%   train_loss=5.35e-05   test_acc=015.8%   test_loss=4.17e+00\n",
      "model #09, epoch #018:   train_acc=100.0%   train_loss=4.67e-05   test_acc=015.8%   test_loss=4.18e+00\n",
      "model #09, epoch #019:   train_acc=100.0%   train_loss=4.13e-05   test_acc=015.8%   test_loss=4.19e+00\n",
      "model #09, epoch #020:   train_acc=100.0%   train_loss=3.69e-05   test_acc=015.9%   test_loss=4.20e+00\n",
      "model #09, epoch #021:   train_acc=100.0%   train_loss=3.32e-05   test_acc=015.9%   test_loss=4.21e+00\n",
      "model #09, epoch #022:   train_acc=100.0%   train_loss=3.00e-05   test_acc=015.9%   test_loss=4.22e+00\n",
      "model #09, epoch #023:   train_acc=100.0%   train_loss=2.73e-05   test_acc=015.8%   test_loss=4.23e+00\n",
      "model #09, epoch #024:   train_acc=100.0%   train_loss=2.50e-05   test_acc=015.9%   test_loss=4.24e+00\n",
      "model #09, epoch #025:   train_acc=100.0%   train_loss=2.30e-05   test_acc=015.8%   test_loss=4.24e+00\n",
      "model #09, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=015.8%   test_loss=4.25e+00\n",
      "model #09, epoch #027:   train_acc=100.0%   train_loss=1.96e-05   test_acc=015.7%   test_loss=4.26e+00\n",
      "model #09, epoch #028:   train_acc=100.0%   train_loss=1.82e-05   test_acc=015.7%   test_loss=4.27e+00\n",
      "model #09, epoch #029:   train_acc=100.0%   train_loss=1.70e-05   test_acc=015.7%   test_loss=4.28e+00\n",
      "model #09, epoch #030:   train_acc=100.0%   train_loss=1.59e-05   test_acc=015.7%   test_loss=4.29e+00\n",
      "model #09, epoch #031:   train_acc=100.0%   train_loss=1.49e-05   test_acc=015.6%   test_loss=4.29e+00\n",
      "model #09, epoch #032:   train_acc=100.0%   train_loss=1.40e-05   test_acc=015.6%   test_loss=4.30e+00\n",
      "model #09, epoch #033:   train_acc=100.0%   train_loss=1.31e-05   test_acc=015.6%   test_loss=4.31e+00\n",
      "model #09, epoch #034:   train_acc=100.0%   train_loss=1.24e-05   test_acc=015.6%   test_loss=4.31e+00\n",
      "model #09, epoch #035:   train_acc=100.0%   train_loss=1.17e-05   test_acc=015.7%   test_loss=4.32e+00\n",
      "model #09, epoch #036:   train_acc=100.0%   train_loss=1.10e-05   test_acc=015.7%   test_loss=4.33e+00\n",
      "model #09, epoch #037:   train_acc=100.0%   train_loss=1.04e-05   test_acc=015.8%   test_loss=4.33e+00\n",
      "model #09, epoch #038:   train_acc=100.0%   train_loss=9.88e-06   test_acc=015.8%   test_loss=4.34e+00\n",
      "model #09, epoch #039:   train_acc=100.0%   train_loss=9.38e-06   test_acc=015.8%   test_loss=4.35e+00\n",
      "model #09, epoch #040:   train_acc=100.0%   train_loss=8.91e-06   test_acc=015.8%   test_loss=4.35e+00\n",
      "model #09, epoch #041:   train_acc=100.0%   train_loss=8.47e-06   test_acc=015.8%   test_loss=4.36e+00\n",
      "model #09, epoch #042:   train_acc=100.0%   train_loss=8.06e-06   test_acc=015.8%   test_loss=4.37e+00\n",
      "model #09, epoch #043:   train_acc=100.0%   train_loss=7.68e-06   test_acc=015.8%   test_loss=4.37e+00\n",
      "model #09, epoch #044:   train_acc=100.0%   train_loss=7.33e-06   test_acc=015.8%   test_loss=4.38e+00\n",
      "model #09, epoch #045:   train_acc=100.0%   train_loss=6.99e-06   test_acc=015.8%   test_loss=4.39e+00\n",
      "model #09, epoch #046:   train_acc=100.0%   train_loss=6.68e-06   test_acc=015.8%   test_loss=4.39e+00\n",
      "model #09, epoch #047:   train_acc=100.0%   train_loss=6.39e-06   test_acc=015.8%   test_loss=4.40e+00\n",
      "model #09, epoch #048:   train_acc=100.0%   train_loss=6.11e-06   test_acc=015.8%   test_loss=4.40e+00\n",
      "model #09, epoch #049:   train_acc=100.0%   train_loss=5.85e-06   test_acc=015.8%   test_loss=4.41e+00\n",
      "model #10, epoch #000:   train_acc=007.4%   train_loss=1.70e-02   test_acc=010.4%   test_loss=4.06e+00\n",
      "model #10, epoch #001:   train_acc=030.0%   train_loss=1.30e-02   test_acc=013.7%   test_loss=3.83e+00\n",
      "model #10, epoch #002:   train_acc=053.4%   train_loss=9.97e-03   test_acc=014.5%   test_loss=3.75e+00\n",
      "model #10, epoch #003:   train_acc=072.7%   train_loss=7.29e-03   test_acc=015.6%   test_loss=3.73e+00\n",
      "model #10, epoch #004:   train_acc=085.0%   train_loss=5.18e-03   test_acc=015.2%   test_loss=3.75e+00\n",
      "model #10, epoch #005:   train_acc=090.3%   train_loss=3.95e-03   test_acc=015.7%   test_loss=3.80e+00\n",
      "model #10, epoch #006:   train_acc=093.2%   train_loss=3.08e-03   test_acc=015.6%   test_loss=3.85e+00\n",
      "model #10, epoch #007:   train_acc=096.5%   train_loss=2.11e-03   test_acc=015.7%   test_loss=3.89e+00\n",
      "model #10, epoch #008:   train_acc=098.7%   train_loss=1.38e-03   test_acc=015.6%   test_loss=3.95e+00\n",
      "model #10, epoch #009:   train_acc=099.6%   train_loss=7.84e-04   test_acc=015.3%   test_loss=3.99e+00\n",
      "model #10, epoch #010:   train_acc=099.9%   train_loss=4.27e-04   test_acc=016.7%   test_loss=4.01e+00\n",
      "model #10, epoch #011:   train_acc=100.0%   train_loss=2.67e-04   test_acc=016.7%   test_loss=4.06e+00\n",
      "model #10, epoch #012:   train_acc=100.0%   train_loss=1.84e-04   test_acc=016.6%   test_loss=4.08e+00\n",
      "model #10, epoch #013:   train_acc=100.0%   train_loss=1.35e-04   test_acc=016.9%   test_loss=4.11e+00\n",
      "model #10, epoch #014:   train_acc=100.0%   train_loss=1.05e-04   test_acc=017.3%   test_loss=4.12e+00\n",
      "model #10, epoch #015:   train_acc=100.0%   train_loss=8.42e-05   test_acc=017.2%   test_loss=4.13e+00\n",
      "model #10, epoch #016:   train_acc=100.0%   train_loss=6.84e-05   test_acc=017.2%   test_loss=4.15e+00\n",
      "model #10, epoch #017:   train_acc=100.0%   train_loss=5.75e-05   test_acc=017.3%   test_loss=4.16e+00\n",
      "model #10, epoch #018:   train_acc=100.0%   train_loss=4.95e-05   test_acc=017.2%   test_loss=4.17e+00\n",
      "model #10, epoch #019:   train_acc=100.0%   train_loss=4.32e-05   test_acc=017.2%   test_loss=4.19e+00\n",
      "model #10, epoch #020:   train_acc=100.0%   train_loss=3.81e-05   test_acc=017.1%   test_loss=4.20e+00\n",
      "model #10, epoch #021:   train_acc=100.0%   train_loss=3.40e-05   test_acc=017.1%   test_loss=4.21e+00\n",
      "model #10, epoch #022:   train_acc=100.0%   train_loss=3.06e-05   test_acc=017.1%   test_loss=4.22e+00\n",
      "model #10, epoch #023:   train_acc=100.0%   train_loss=2.77e-05   test_acc=017.1%   test_loss=4.23e+00\n",
      "model #10, epoch #024:   train_acc=100.0%   train_loss=2.52e-05   test_acc=017.2%   test_loss=4.24e+00\n",
      "model #10, epoch #025:   train_acc=100.0%   train_loss=2.31e-05   test_acc=017.1%   test_loss=4.25e+00\n",
      "model #10, epoch #026:   train_acc=100.0%   train_loss=2.13e-05   test_acc=017.0%   test_loss=4.26e+00\n",
      "model #10, epoch #027:   train_acc=100.0%   train_loss=1.97e-05   test_acc=017.1%   test_loss=4.27e+00\n",
      "model #10, epoch #028:   train_acc=100.0%   train_loss=1.82e-05   test_acc=017.1%   test_loss=4.28e+00\n",
      "model #10, epoch #029:   train_acc=100.0%   train_loss=1.70e-05   test_acc=017.1%   test_loss=4.29e+00\n",
      "model #10, epoch #030:   train_acc=100.0%   train_loss=1.59e-05   test_acc=017.1%   test_loss=4.30e+00\n",
      "model #10, epoch #031:   train_acc=100.0%   train_loss=1.48e-05   test_acc=017.1%   test_loss=4.30e+00\n",
      "model #10, epoch #032:   train_acc=100.0%   train_loss=1.39e-05   test_acc=017.0%   test_loss=4.31e+00\n",
      "model #10, epoch #033:   train_acc=100.0%   train_loss=1.31e-05   test_acc=017.0%   test_loss=4.32e+00\n",
      "model #10, epoch #034:   train_acc=100.0%   train_loss=1.23e-05   test_acc=017.0%   test_loss=4.33e+00\n",
      "model #10, epoch #035:   train_acc=100.0%   train_loss=1.16e-05   test_acc=017.0%   test_loss=4.33e+00\n",
      "model #10, epoch #036:   train_acc=100.0%   train_loss=1.10e-05   test_acc=017.0%   test_loss=4.34e+00\n",
      "model #10, epoch #037:   train_acc=100.0%   train_loss=1.04e-05   test_acc=017.0%   test_loss=4.35e+00\n",
      "model #10, epoch #038:   train_acc=100.0%   train_loss=9.85e-06   test_acc=016.9%   test_loss=4.36e+00\n",
      "model #10, epoch #039:   train_acc=100.0%   train_loss=9.34e-06   test_acc=017.0%   test_loss=4.36e+00\n",
      "model #10, epoch #040:   train_acc=100.0%   train_loss=8.88e-06   test_acc=017.0%   test_loss=4.37e+00\n",
      "model #10, epoch #041:   train_acc=100.0%   train_loss=8.44e-06   test_acc=017.0%   test_loss=4.38e+00\n",
      "model #10, epoch #042:   train_acc=100.0%   train_loss=8.03e-06   test_acc=016.9%   test_loss=4.38e+00\n",
      "model #10, epoch #043:   train_acc=100.0%   train_loss=7.65e-06   test_acc=016.9%   test_loss=4.39e+00\n",
      "model #10, epoch #044:   train_acc=100.0%   train_loss=7.30e-06   test_acc=017.0%   test_loss=4.40e+00\n",
      "model #10, epoch #045:   train_acc=100.0%   train_loss=6.97e-06   test_acc=017.0%   test_loss=4.40e+00\n",
      "model #10, epoch #046:   train_acc=100.0%   train_loss=6.66e-06   test_acc=017.0%   test_loss=4.41e+00\n",
      "model #10, epoch #047:   train_acc=100.0%   train_loss=6.36e-06   test_acc=017.0%   test_loss=4.41e+00\n",
      "model #10, epoch #048:   train_acc=100.0%   train_loss=6.09e-06   test_acc=016.9%   test_loss=4.42e+00\n",
      "model #10, epoch #049:   train_acc=100.0%   train_loss=5.83e-06   test_acc=017.0%   test_loss=4.43e+00\n",
      "model #11, epoch #000:   train_acc=007.5%   train_loss=1.69e-02   test_acc=009.6%   test_loss=4.07e+00\n",
      "model #11, epoch #001:   train_acc=031.3%   train_loss=1.30e-02   test_acc=011.7%   test_loss=3.87e+00\n",
      "model #11, epoch #002:   train_acc=054.3%   train_loss=9.91e-03   test_acc=013.8%   test_loss=3.80e+00\n",
      "model #11, epoch #003:   train_acc=072.9%   train_loss=7.21e-03   test_acc=014.4%   test_loss=3.78e+00\n",
      "model #11, epoch #004:   train_acc=085.0%   train_loss=5.11e-03   test_acc=014.4%   test_loss=3.82e+00\n",
      "model #11, epoch #005:   train_acc=090.5%   train_loss=3.84e-03   test_acc=015.1%   test_loss=3.84e+00\n",
      "model #11, epoch #006:   train_acc=094.6%   train_loss=2.78e-03   test_acc=015.4%   test_loss=3.87e+00\n",
      "model #11, epoch #007:   train_acc=098.0%   train_loss=1.77e-03   test_acc=014.8%   test_loss=3.97e+00\n",
      "model #11, epoch #008:   train_acc=099.3%   train_loss=1.10e-03   test_acc=015.2%   test_loss=4.02e+00\n",
      "model #11, epoch #009:   train_acc=099.9%   train_loss=6.74e-04   test_acc=015.2%   test_loss=4.04e+00\n",
      "model #11, epoch #010:   train_acc=100.0%   train_loss=4.22e-04   test_acc=016.4%   test_loss=4.06e+00\n",
      "model #11, epoch #011:   train_acc=100.0%   train_loss=2.60e-04   test_acc=016.0%   test_loss=4.10e+00\n",
      "model #11, epoch #012:   train_acc=100.0%   train_loss=1.64e-04   test_acc=016.2%   test_loss=4.11e+00\n",
      "model #11, epoch #013:   train_acc=100.0%   train_loss=1.21e-04   test_acc=016.2%   test_loss=4.13e+00\n",
      "model #11, epoch #014:   train_acc=100.0%   train_loss=9.08e-05   test_acc=016.5%   test_loss=4.15e+00\n",
      "model #11, epoch #015:   train_acc=100.0%   train_loss=7.25e-05   test_acc=016.4%   test_loss=4.17e+00\n",
      "model #11, epoch #016:   train_acc=100.0%   train_loss=6.07e-05   test_acc=016.6%   test_loss=4.18e+00\n",
      "model #11, epoch #017:   train_acc=100.0%   train_loss=5.23e-05   test_acc=016.3%   test_loss=4.20e+00\n",
      "model #11, epoch #018:   train_acc=100.0%   train_loss=4.58e-05   test_acc=016.3%   test_loss=4.21e+00\n",
      "model #11, epoch #019:   train_acc=100.0%   train_loss=4.06e-05   test_acc=016.4%   test_loss=4.23e+00\n",
      "model #11, epoch #020:   train_acc=100.0%   train_loss=3.64e-05   test_acc=016.5%   test_loss=4.24e+00\n",
      "model #11, epoch #021:   train_acc=100.0%   train_loss=3.28e-05   test_acc=016.4%   test_loss=4.25e+00\n",
      "model #11, epoch #022:   train_acc=100.0%   train_loss=2.98e-05   test_acc=016.4%   test_loss=4.26e+00\n",
      "model #11, epoch #023:   train_acc=100.0%   train_loss=2.72e-05   test_acc=016.4%   test_loss=4.27e+00\n",
      "model #11, epoch #024:   train_acc=100.0%   train_loss=2.49e-05   test_acc=016.5%   test_loss=4.28e+00\n",
      "model #11, epoch #025:   train_acc=100.0%   train_loss=2.30e-05   test_acc=016.4%   test_loss=4.29e+00\n",
      "model #11, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=016.4%   test_loss=4.30e+00\n",
      "model #11, epoch #027:   train_acc=100.0%   train_loss=1.97e-05   test_acc=016.6%   test_loss=4.31e+00\n",
      "model #11, epoch #028:   train_acc=100.0%   train_loss=1.83e-05   test_acc=016.5%   test_loss=4.32e+00\n",
      "model #11, epoch #029:   train_acc=100.0%   train_loss=1.71e-05   test_acc=016.4%   test_loss=4.32e+00\n",
      "model #11, epoch #030:   train_acc=100.0%   train_loss=1.60e-05   test_acc=016.4%   test_loss=4.33e+00\n",
      "model #11, epoch #031:   train_acc=100.0%   train_loss=1.50e-05   test_acc=016.4%   test_loss=4.34e+00\n",
      "model #11, epoch #032:   train_acc=100.0%   train_loss=1.41e-05   test_acc=016.3%   test_loss=4.35e+00\n",
      "model #11, epoch #033:   train_acc=100.0%   train_loss=1.32e-05   test_acc=016.2%   test_loss=4.36e+00\n",
      "model #11, epoch #034:   train_acc=100.0%   train_loss=1.24e-05   test_acc=016.2%   test_loss=4.36e+00\n",
      "model #11, epoch #035:   train_acc=100.0%   train_loss=1.17e-05   test_acc=016.2%   test_loss=4.37e+00\n",
      "model #11, epoch #036:   train_acc=100.0%   train_loss=1.11e-05   test_acc=016.2%   test_loss=4.38e+00\n",
      "model #11, epoch #037:   train_acc=100.0%   train_loss=1.05e-05   test_acc=016.2%   test_loss=4.38e+00\n",
      "model #11, epoch #038:   train_acc=100.0%   train_loss=9.93e-06   test_acc=016.2%   test_loss=4.39e+00\n",
      "model #11, epoch #039:   train_acc=100.0%   train_loss=9.42e-06   test_acc=016.1%   test_loss=4.40e+00\n",
      "model #11, epoch #040:   train_acc=100.0%   train_loss=8.94e-06   test_acc=016.1%   test_loss=4.40e+00\n",
      "model #11, epoch #041:   train_acc=100.0%   train_loss=8.49e-06   test_acc=016.1%   test_loss=4.41e+00\n",
      "model #11, epoch #042:   train_acc=100.0%   train_loss=8.08e-06   test_acc=016.0%   test_loss=4.42e+00\n",
      "model #11, epoch #043:   train_acc=100.0%   train_loss=7.69e-06   test_acc=015.9%   test_loss=4.42e+00\n",
      "model #11, epoch #044:   train_acc=100.0%   train_loss=7.32e-06   test_acc=016.0%   test_loss=4.43e+00\n",
      "model #11, epoch #045:   train_acc=100.0%   train_loss=6.98e-06   test_acc=016.0%   test_loss=4.44e+00\n",
      "model #11, epoch #046:   train_acc=100.0%   train_loss=6.67e-06   test_acc=016.0%   test_loss=4.44e+00\n",
      "model #11, epoch #047:   train_acc=100.0%   train_loss=6.37e-06   test_acc=016.0%   test_loss=4.45e+00\n",
      "model #11, epoch #048:   train_acc=100.0%   train_loss=6.08e-06   test_acc=016.0%   test_loss=4.45e+00\n",
      "model #11, epoch #049:   train_acc=100.0%   train_loss=5.82e-06   test_acc=016.0%   test_loss=4.46e+00\n",
      "model #12, epoch #000:   train_acc=008.0%   train_loss=1.70e-02   test_acc=009.8%   test_loss=4.09e+00\n",
      "model #12, epoch #001:   train_acc=029.9%   train_loss=1.31e-02   test_acc=013.9%   test_loss=3.88e+00\n",
      "model #12, epoch #002:   train_acc=053.2%   train_loss=1.01e-02   test_acc=015.4%   test_loss=3.80e+00\n",
      "model #12, epoch #003:   train_acc=071.6%   train_loss=7.43e-03   test_acc=016.1%   test_loss=3.80e+00\n",
      "model #12, epoch #004:   train_acc=084.0%   train_loss=5.31e-03   test_acc=015.6%   test_loss=3.82e+00\n",
      "model #12, epoch #005:   train_acc=089.8%   train_loss=3.97e-03   test_acc=015.5%   test_loss=3.86e+00\n",
      "model #12, epoch #006:   train_acc=094.3%   train_loss=2.88e-03   test_acc=015.0%   test_loss=3.92e+00\n",
      "model #12, epoch #007:   train_acc=097.3%   train_loss=1.95e-03   test_acc=015.5%   test_loss=3.97e+00\n",
      "model #12, epoch #008:   train_acc=098.8%   train_loss=1.30e-03   test_acc=015.6%   test_loss=4.01e+00\n",
      "model #12, epoch #009:   train_acc=099.7%   train_loss=7.79e-04   test_acc=015.3%   test_loss=4.07e+00\n",
      "model #12, epoch #010:   train_acc=100.0%   train_loss=4.63e-04   test_acc=015.8%   test_loss=4.11e+00\n",
      "model #12, epoch #011:   train_acc=100.0%   train_loss=2.83e-04   test_acc=015.7%   test_loss=4.14e+00\n",
      "model #12, epoch #012:   train_acc=100.0%   train_loss=1.82e-04   test_acc=015.9%   test_loss=4.15e+00\n",
      "model #12, epoch #013:   train_acc=100.0%   train_loss=1.26e-04   test_acc=015.9%   test_loss=4.17e+00\n",
      "model #12, epoch #014:   train_acc=100.0%   train_loss=9.54e-05   test_acc=015.9%   test_loss=4.19e+00\n",
      "model #12, epoch #015:   train_acc=100.0%   train_loss=7.67e-05   test_acc=016.4%   test_loss=4.20e+00\n",
      "model #12, epoch #016:   train_acc=100.0%   train_loss=6.41e-05   test_acc=016.2%   test_loss=4.21e+00\n",
      "model #12, epoch #017:   train_acc=100.0%   train_loss=5.48e-05   test_acc=016.4%   test_loss=4.23e+00\n",
      "model #12, epoch #018:   train_acc=100.0%   train_loss=4.77e-05   test_acc=016.2%   test_loss=4.24e+00\n",
      "model #12, epoch #019:   train_acc=100.0%   train_loss=4.20e-05   test_acc=016.2%   test_loss=4.25e+00\n",
      "model #12, epoch #020:   train_acc=100.0%   train_loss=3.74e-05   test_acc=016.3%   test_loss=4.26e+00\n",
      "model #12, epoch #021:   train_acc=100.0%   train_loss=3.36e-05   test_acc=016.3%   test_loss=4.27e+00\n",
      "model #12, epoch #022:   train_acc=100.0%   train_loss=3.04e-05   test_acc=016.3%   test_loss=4.28e+00\n",
      "model #12, epoch #023:   train_acc=100.0%   train_loss=2.76e-05   test_acc=016.2%   test_loss=4.29e+00\n",
      "model #12, epoch #024:   train_acc=100.0%   train_loss=2.53e-05   test_acc=016.4%   test_loss=4.30e+00\n",
      "model #12, epoch #025:   train_acc=100.0%   train_loss=2.32e-05   test_acc=016.4%   test_loss=4.31e+00\n",
      "model #12, epoch #026:   train_acc=100.0%   train_loss=2.14e-05   test_acc=016.3%   test_loss=4.32e+00\n",
      "model #12, epoch #027:   train_acc=100.0%   train_loss=1.98e-05   test_acc=016.3%   test_loss=4.33e+00\n",
      "model #12, epoch #028:   train_acc=100.0%   train_loss=1.84e-05   test_acc=016.4%   test_loss=4.34e+00\n",
      "model #12, epoch #029:   train_acc=100.0%   train_loss=1.71e-05   test_acc=016.4%   test_loss=4.35e+00\n",
      "model #12, epoch #030:   train_acc=100.0%   train_loss=1.60e-05   test_acc=016.4%   test_loss=4.36e+00\n",
      "model #12, epoch #031:   train_acc=100.0%   train_loss=1.50e-05   test_acc=016.3%   test_loss=4.36e+00\n",
      "model #12, epoch #032:   train_acc=100.0%   train_loss=1.40e-05   test_acc=016.3%   test_loss=4.37e+00\n",
      "model #12, epoch #033:   train_acc=100.0%   train_loss=1.32e-05   test_acc=016.3%   test_loss=4.38e+00\n",
      "model #12, epoch #034:   train_acc=100.0%   train_loss=1.24e-05   test_acc=016.3%   test_loss=4.39e+00\n",
      "model #12, epoch #035:   train_acc=100.0%   train_loss=1.17e-05   test_acc=016.2%   test_loss=4.39e+00\n",
      "model #12, epoch #036:   train_acc=100.0%   train_loss=1.11e-05   test_acc=016.2%   test_loss=4.40e+00\n",
      "model #12, epoch #037:   train_acc=100.0%   train_loss=1.05e-05   test_acc=016.2%   test_loss=4.41e+00\n",
      "model #12, epoch #038:   train_acc=100.0%   train_loss=9.91e-06   test_acc=016.2%   test_loss=4.41e+00\n",
      "model #12, epoch #039:   train_acc=100.0%   train_loss=9.40e-06   test_acc=016.2%   test_loss=4.42e+00\n",
      "model #12, epoch #040:   train_acc=100.0%   train_loss=8.92e-06   test_acc=016.2%   test_loss=4.43e+00\n",
      "model #12, epoch #041:   train_acc=100.0%   train_loss=8.48e-06   test_acc=016.2%   test_loss=4.43e+00\n",
      "model #12, epoch #042:   train_acc=100.0%   train_loss=8.07e-06   test_acc=016.2%   test_loss=4.44e+00\n",
      "model #12, epoch #043:   train_acc=100.0%   train_loss=7.69e-06   test_acc=016.2%   test_loss=4.45e+00\n",
      "model #12, epoch #044:   train_acc=100.0%   train_loss=7.33e-06   test_acc=016.1%   test_loss=4.45e+00\n",
      "model #12, epoch #045:   train_acc=100.0%   train_loss=6.99e-06   test_acc=016.1%   test_loss=4.46e+00\n",
      "model #12, epoch #046:   train_acc=100.0%   train_loss=6.68e-06   test_acc=016.2%   test_loss=4.46e+00\n",
      "model #12, epoch #047:   train_acc=100.0%   train_loss=6.38e-06   test_acc=016.2%   test_loss=4.47e+00\n",
      "model #12, epoch #048:   train_acc=100.0%   train_loss=6.10e-06   test_acc=016.2%   test_loss=4.48e+00\n",
      "model #12, epoch #049:   train_acc=100.0%   train_loss=5.84e-06   test_acc=016.2%   test_loss=4.48e+00\n",
      "model #13, epoch #000:   train_acc=008.4%   train_loss=1.68e-02   test_acc=010.1%   test_loss=4.04e+00\n",
      "model #13, epoch #001:   train_acc=031.1%   train_loss=1.28e-02   test_acc=012.5%   test_loss=3.85e+00\n",
      "model #13, epoch #002:   train_acc=054.3%   train_loss=9.75e-03   test_acc=014.8%   test_loss=3.78e+00\n",
      "model #13, epoch #003:   train_acc=073.8%   train_loss=7.04e-03   test_acc=015.3%   test_loss=3.78e+00\n",
      "model #13, epoch #004:   train_acc=086.3%   train_loss=4.94e-03   test_acc=014.3%   test_loss=3.81e+00\n",
      "model #13, epoch #005:   train_acc=091.9%   train_loss=3.63e-03   test_acc=014.9%   test_loss=3.86e+00\n",
      "model #13, epoch #006:   train_acc=095.5%   train_loss=2.63e-03   test_acc=015.2%   test_loss=3.91e+00\n",
      "model #13, epoch #007:   train_acc=098.1%   train_loss=1.81e-03   test_acc=015.0%   test_loss=3.99e+00\n",
      "model #13, epoch #008:   train_acc=099.1%   train_loss=1.24e-03   test_acc=014.9%   test_loss=4.04e+00\n",
      "model #13, epoch #009:   train_acc=099.7%   train_loss=7.49e-04   test_acc=015.4%   test_loss=4.09e+00\n",
      "model #13, epoch #010:   train_acc=099.9%   train_loss=4.16e-04   test_acc=016.0%   test_loss=4.11e+00\n",
      "model #13, epoch #011:   train_acc=100.0%   train_loss=2.46e-04   test_acc=015.7%   test_loss=4.13e+00\n",
      "model #13, epoch #012:   train_acc=100.0%   train_loss=1.56e-04   test_acc=016.2%   test_loss=4.16e+00\n",
      "model #13, epoch #013:   train_acc=100.0%   train_loss=1.15e-04   test_acc=015.9%   test_loss=4.19e+00\n",
      "model #13, epoch #014:   train_acc=100.0%   train_loss=8.97e-05   test_acc=016.3%   test_loss=4.21e+00\n",
      "model #13, epoch #015:   train_acc=100.0%   train_loss=7.28e-05   test_acc=016.3%   test_loss=4.23e+00\n",
      "model #13, epoch #016:   train_acc=100.0%   train_loss=6.14e-05   test_acc=016.1%   test_loss=4.24e+00\n",
      "model #13, epoch #017:   train_acc=100.0%   train_loss=5.27e-05   test_acc=016.3%   test_loss=4.26e+00\n",
      "model #13, epoch #018:   train_acc=100.0%   train_loss=4.59e-05   test_acc=016.3%   test_loss=4.27e+00\n",
      "model #13, epoch #019:   train_acc=100.0%   train_loss=4.03e-05   test_acc=016.1%   test_loss=4.28e+00\n",
      "model #13, epoch #020:   train_acc=100.0%   train_loss=3.58e-05   test_acc=016.4%   test_loss=4.29e+00\n",
      "model #13, epoch #021:   train_acc=100.0%   train_loss=3.21e-05   test_acc=016.3%   test_loss=4.30e+00\n",
      "model #13, epoch #022:   train_acc=100.0%   train_loss=2.90e-05   test_acc=016.3%   test_loss=4.31e+00\n",
      "model #13, epoch #023:   train_acc=100.0%   train_loss=2.64e-05   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #13, epoch #024:   train_acc=100.0%   train_loss=2.46e-05   test_acc=016.3%   test_loss=4.32e+00\n",
      "model #13, epoch #025:   train_acc=100.0%   train_loss=2.25e-05   test_acc=016.3%   test_loss=4.34e+00\n",
      "model #13, epoch #026:   train_acc=100.0%   train_loss=2.08e-05   test_acc=016.5%   test_loss=4.34e+00\n",
      "model #13, epoch #027:   train_acc=100.0%   train_loss=1.91e-05   test_acc=016.5%   test_loss=4.35e+00\n",
      "model #13, epoch #028:   train_acc=100.0%   train_loss=1.78e-05   test_acc=016.5%   test_loss=4.36e+00\n",
      "model #13, epoch #029:   train_acc=100.0%   train_loss=1.65e-05   test_acc=016.4%   test_loss=4.37e+00\n",
      "model #13, epoch #030:   train_acc=100.0%   train_loss=1.55e-05   test_acc=016.5%   test_loss=4.37e+00\n",
      "model #13, epoch #031:   train_acc=100.0%   train_loss=1.44e-05   test_acc=016.3%   test_loss=4.38e+00\n",
      "model #13, epoch #032:   train_acc=100.0%   train_loss=1.34e-05   test_acc=016.4%   test_loss=4.39e+00\n",
      "model #13, epoch #033:   train_acc=100.0%   train_loss=1.26e-05   test_acc=016.4%   test_loss=4.40e+00\n",
      "model #13, epoch #034:   train_acc=100.0%   train_loss=1.19e-05   test_acc=016.4%   test_loss=4.40e+00\n",
      "model #13, epoch #035:   train_acc=100.0%   train_loss=1.12e-05   test_acc=016.5%   test_loss=4.41e+00\n",
      "model #13, epoch #036:   train_acc=100.0%   train_loss=1.06e-05   test_acc=016.5%   test_loss=4.42e+00\n",
      "model #13, epoch #037:   train_acc=100.0%   train_loss=1.00e-05   test_acc=016.5%   test_loss=4.42e+00\n",
      "model #13, epoch #038:   train_acc=100.0%   train_loss=9.51e-06   test_acc=016.5%   test_loss=4.43e+00\n",
      "model #13, epoch #039:   train_acc=100.0%   train_loss=9.03e-06   test_acc=016.4%   test_loss=4.44e+00\n",
      "model #13, epoch #040:   train_acc=100.0%   train_loss=8.58e-06   test_acc=016.5%   test_loss=4.44e+00\n",
      "model #13, epoch #041:   train_acc=100.0%   train_loss=8.17e-06   test_acc=016.6%   test_loss=4.45e+00\n",
      "model #13, epoch #042:   train_acc=100.0%   train_loss=7.78e-06   test_acc=016.6%   test_loss=4.46e+00\n",
      "model #13, epoch #043:   train_acc=100.0%   train_loss=7.41e-06   test_acc=016.6%   test_loss=4.46e+00\n",
      "model #13, epoch #044:   train_acc=100.0%   train_loss=7.07e-06   test_acc=016.7%   test_loss=4.47e+00\n",
      "model #13, epoch #045:   train_acc=100.0%   train_loss=6.75e-06   test_acc=016.7%   test_loss=4.47e+00\n",
      "model #13, epoch #046:   train_acc=100.0%   train_loss=6.45e-06   test_acc=016.7%   test_loss=4.48e+00\n",
      "model #13, epoch #047:   train_acc=100.0%   train_loss=6.17e-06   test_acc=016.7%   test_loss=4.49e+00\n",
      "model #13, epoch #048:   train_acc=100.0%   train_loss=5.90e-06   test_acc=016.7%   test_loss=4.49e+00\n",
      "model #13, epoch #049:   train_acc=100.0%   train_loss=5.65e-06   test_acc=016.7%   test_loss=4.50e+00\n",
      "model #14, epoch #000:   train_acc=007.6%   train_loss=1.69e-02   test_acc=009.5%   test_loss=4.09e+00\n",
      "model #14, epoch #001:   train_acc=030.4%   train_loss=1.30e-02   test_acc=012.2%   test_loss=3.89e+00\n",
      "model #14, epoch #002:   train_acc=054.0%   train_loss=9.91e-03   test_acc=013.6%   test_loss=3.82e+00\n",
      "model #14, epoch #003:   train_acc=072.4%   train_loss=7.22e-03   test_acc=014.2%   test_loss=3.81e+00\n",
      "model #14, epoch #004:   train_acc=085.8%   train_loss=5.06e-03   test_acc=014.7%   test_loss=3.84e+00\n",
      "model #14, epoch #005:   train_acc=092.0%   train_loss=3.71e-03   test_acc=014.1%   test_loss=3.89e+00\n",
      "model #14, epoch #006:   train_acc=095.3%   train_loss=2.80e-03   test_acc=014.8%   test_loss=3.93e+00\n",
      "model #14, epoch #007:   train_acc=097.7%   train_loss=1.90e-03   test_acc=014.9%   test_loss=3.99e+00\n",
      "model #14, epoch #008:   train_acc=099.3%   train_loss=1.20e-03   test_acc=014.5%   test_loss=4.01e+00\n",
      "model #14, epoch #009:   train_acc=099.9%   train_loss=6.70e-04   test_acc=014.8%   test_loss=4.06e+00\n",
      "model #14, epoch #010:   train_acc=100.0%   train_loss=3.90e-04   test_acc=015.6%   test_loss=4.10e+00\n",
      "model #14, epoch #011:   train_acc=100.0%   train_loss=2.61e-04   test_acc=015.4%   test_loss=4.12e+00\n",
      "model #14, epoch #012:   train_acc=100.0%   train_loss=1.78e-04   test_acc=015.7%   test_loss=4.14e+00\n",
      "model #14, epoch #013:   train_acc=100.0%   train_loss=1.26e-04   test_acc=015.7%   test_loss=4.16e+00\n",
      "model #14, epoch #014:   train_acc=100.0%   train_loss=9.58e-05   test_acc=015.3%   test_loss=4.18e+00\n",
      "model #14, epoch #015:   train_acc=100.0%   train_loss=7.93e-05   test_acc=015.4%   test_loss=4.20e+00\n",
      "model #14, epoch #016:   train_acc=100.0%   train_loss=6.73e-05   test_acc=015.3%   test_loss=4.22e+00\n",
      "model #14, epoch #017:   train_acc=100.0%   train_loss=5.46e-05   test_acc=015.2%   test_loss=4.23e+00\n",
      "model #14, epoch #018:   train_acc=100.0%   train_loss=4.85e-05   test_acc=015.4%   test_loss=4.24e+00\n",
      "model #14, epoch #019:   train_acc=100.0%   train_loss=4.19e-05   test_acc=015.6%   test_loss=4.25e+00\n",
      "model #14, epoch #020:   train_acc=100.0%   train_loss=3.66e-05   test_acc=015.5%   test_loss=4.27e+00\n",
      "model #14, epoch #021:   train_acc=100.0%   train_loss=3.27e-05   test_acc=015.6%   test_loss=4.28e+00\n",
      "model #14, epoch #022:   train_acc=100.0%   train_loss=2.95e-05   test_acc=015.6%   test_loss=4.29e+00\n",
      "model #14, epoch #023:   train_acc=100.0%   train_loss=2.68e-05   test_acc=015.5%   test_loss=4.30e+00\n",
      "model #14, epoch #024:   train_acc=100.0%   train_loss=2.46e-05   test_acc=015.5%   test_loss=4.31e+00\n",
      "model #14, epoch #025:   train_acc=100.0%   train_loss=2.26e-05   test_acc=015.4%   test_loss=4.31e+00\n",
      "model #14, epoch #026:   train_acc=100.0%   train_loss=2.08e-05   test_acc=015.5%   test_loss=4.32e+00\n",
      "model #14, epoch #027:   train_acc=100.0%   train_loss=1.93e-05   test_acc=015.5%   test_loss=4.33e+00\n",
      "model #14, epoch #028:   train_acc=100.0%   train_loss=1.79e-05   test_acc=015.4%   test_loss=4.34e+00\n",
      "model #14, epoch #029:   train_acc=100.0%   train_loss=1.67e-05   test_acc=015.5%   test_loss=4.35e+00\n",
      "model #14, epoch #030:   train_acc=100.0%   train_loss=1.56e-05   test_acc=015.5%   test_loss=4.36e+00\n",
      "model #14, epoch #031:   train_acc=100.0%   train_loss=1.46e-05   test_acc=015.5%   test_loss=4.37e+00\n",
      "model #14, epoch #032:   train_acc=100.0%   train_loss=1.37e-05   test_acc=015.5%   test_loss=4.37e+00\n",
      "model #14, epoch #033:   train_acc=100.0%   train_loss=1.29e-05   test_acc=015.4%   test_loss=4.38e+00\n",
      "model #14, epoch #034:   train_acc=100.0%   train_loss=1.21e-05   test_acc=015.4%   test_loss=4.39e+00\n",
      "model #14, epoch #035:   train_acc=100.0%   train_loss=1.14e-05   test_acc=015.4%   test_loss=4.39e+00\n",
      "model #14, epoch #036:   train_acc=100.0%   train_loss=1.08e-05   test_acc=015.3%   test_loss=4.40e+00\n",
      "model #14, epoch #037:   train_acc=100.0%   train_loss=1.02e-05   test_acc=015.3%   test_loss=4.41e+00\n",
      "model #14, epoch #038:   train_acc=100.0%   train_loss=9.67e-06   test_acc=015.3%   test_loss=4.41e+00\n",
      "model #14, epoch #039:   train_acc=100.0%   train_loss=9.18e-06   test_acc=015.4%   test_loss=4.42e+00\n",
      "model #14, epoch #040:   train_acc=100.0%   train_loss=8.71e-06   test_acc=015.3%   test_loss=4.43e+00\n",
      "model #14, epoch #041:   train_acc=100.0%   train_loss=8.28e-06   test_acc=015.3%   test_loss=4.43e+00\n",
      "model #14, epoch #042:   train_acc=100.0%   train_loss=7.88e-06   test_acc=015.3%   test_loss=4.44e+00\n",
      "model #14, epoch #043:   train_acc=100.0%   train_loss=7.51e-06   test_acc=015.4%   test_loss=4.45e+00\n",
      "model #14, epoch #044:   train_acc=100.0%   train_loss=7.16e-06   test_acc=015.4%   test_loss=4.45e+00\n",
      "model #14, epoch #045:   train_acc=100.0%   train_loss=6.83e-06   test_acc=015.4%   test_loss=4.46e+00\n",
      "model #14, epoch #046:   train_acc=100.0%   train_loss=6.53e-06   test_acc=015.4%   test_loss=4.46e+00\n",
      "model #14, epoch #047:   train_acc=100.0%   train_loss=6.24e-06   test_acc=015.4%   test_loss=4.47e+00\n",
      "model #14, epoch #048:   train_acc=100.0%   train_loss=5.97e-06   test_acc=015.4%   test_loss=4.48e+00\n",
      "model #14, epoch #049:   train_acc=100.0%   train_loss=5.71e-06   test_acc=015.4%   test_loss=4.48e+00\n",
      "model #15, epoch #000:   train_acc=007.4%   train_loss=1.70e-02   test_acc=009.8%   test_loss=4.06e+00\n",
      "model #15, epoch #001:   train_acc=030.2%   train_loss=1.30e-02   test_acc=012.9%   test_loss=3.85e+00\n",
      "model #15, epoch #002:   train_acc=054.3%   train_loss=9.95e-03   test_acc=014.3%   test_loss=3.76e+00\n",
      "model #15, epoch #003:   train_acc=073.3%   train_loss=7.24e-03   test_acc=015.0%   test_loss=3.75e+00\n",
      "model #15, epoch #004:   train_acc=085.7%   train_loss=5.15e-03   test_acc=015.1%   test_loss=3.76e+00\n",
      "model #15, epoch #005:   train_acc=091.4%   train_loss=3.80e-03   test_acc=015.2%   test_loss=3.80e+00\n",
      "model #15, epoch #006:   train_acc=095.2%   train_loss=2.69e-03   test_acc=015.6%   test_loss=3.85e+00\n",
      "model #15, epoch #007:   train_acc=098.5%   train_loss=1.64e-03   test_acc=015.8%   test_loss=3.91e+00\n",
      "model #15, epoch #008:   train_acc=099.7%   train_loss=9.87e-04   test_acc=015.6%   test_loss=3.96e+00\n",
      "model #15, epoch #009:   train_acc=099.9%   train_loss=6.21e-04   test_acc=015.7%   test_loss=3.99e+00\n",
      "model #15, epoch #010:   train_acc=100.0%   train_loss=3.96e-04   test_acc=015.6%   test_loss=4.02e+00\n",
      "model #15, epoch #011:   train_acc=100.0%   train_loss=2.56e-04   test_acc=015.5%   test_loss=4.04e+00\n",
      "model #15, epoch #012:   train_acc=100.0%   train_loss=1.66e-04   test_acc=016.2%   test_loss=4.05e+00\n",
      "model #15, epoch #013:   train_acc=100.0%   train_loss=1.20e-04   test_acc=016.4%   test_loss=4.07e+00\n",
      "model #15, epoch #014:   train_acc=100.0%   train_loss=9.32e-05   test_acc=016.4%   test_loss=4.08e+00\n",
      "model #15, epoch #015:   train_acc=100.0%   train_loss=7.55e-05   test_acc=016.0%   test_loss=4.10e+00\n",
      "model #15, epoch #016:   train_acc=100.0%   train_loss=6.30e-05   test_acc=015.9%   test_loss=4.11e+00\n",
      "model #15, epoch #017:   train_acc=100.0%   train_loss=5.37e-05   test_acc=016.1%   test_loss=4.13e+00\n",
      "model #15, epoch #018:   train_acc=100.0%   train_loss=4.65e-05   test_acc=015.9%   test_loss=4.14e+00\n",
      "model #15, epoch #019:   train_acc=100.0%   train_loss=4.09e-05   test_acc=016.0%   test_loss=4.15e+00\n",
      "model #15, epoch #020:   train_acc=100.0%   train_loss=3.64e-05   test_acc=016.2%   test_loss=4.16e+00\n",
      "model #15, epoch #021:   train_acc=100.0%   train_loss=3.26e-05   test_acc=016.2%   test_loss=4.17e+00\n",
      "model #15, epoch #022:   train_acc=100.0%   train_loss=2.95e-05   test_acc=016.3%   test_loss=4.18e+00\n",
      "model #15, epoch #023:   train_acc=100.0%   train_loss=2.68e-05   test_acc=016.3%   test_loss=4.19e+00\n",
      "model #15, epoch #024:   train_acc=100.0%   train_loss=2.45e-05   test_acc=016.4%   test_loss=4.20e+00\n",
      "model #15, epoch #025:   train_acc=100.0%   train_loss=2.25e-05   test_acc=016.3%   test_loss=4.21e+00\n",
      "model #15, epoch #026:   train_acc=100.0%   train_loss=2.08e-05   test_acc=016.3%   test_loss=4.22e+00\n",
      "model #15, epoch #027:   train_acc=100.0%   train_loss=1.92e-05   test_acc=016.3%   test_loss=4.23e+00\n",
      "model #15, epoch #028:   train_acc=100.0%   train_loss=1.79e-05   test_acc=016.3%   test_loss=4.23e+00\n",
      "model #15, epoch #029:   train_acc=100.0%   train_loss=1.66e-05   test_acc=016.4%   test_loss=4.24e+00\n",
      "model #15, epoch #030:   train_acc=100.0%   train_loss=1.55e-05   test_acc=016.3%   test_loss=4.25e+00\n",
      "model #15, epoch #031:   train_acc=100.0%   train_loss=1.45e-05   test_acc=016.3%   test_loss=4.26e+00\n",
      "model #15, epoch #032:   train_acc=100.0%   train_loss=1.37e-05   test_acc=016.3%   test_loss=4.26e+00\n",
      "model #15, epoch #033:   train_acc=100.0%   train_loss=1.28e-05   test_acc=016.4%   test_loss=4.27e+00\n",
      "model #15, epoch #034:   train_acc=100.0%   train_loss=1.21e-05   test_acc=016.3%   test_loss=4.28e+00\n",
      "model #15, epoch #035:   train_acc=100.0%   train_loss=1.14e-05   test_acc=016.3%   test_loss=4.29e+00\n",
      "model #15, epoch #036:   train_acc=100.0%   train_loss=1.08e-05   test_acc=016.3%   test_loss=4.29e+00\n",
      "model #15, epoch #037:   train_acc=100.0%   train_loss=1.02e-05   test_acc=016.3%   test_loss=4.30e+00\n",
      "model #15, epoch #038:   train_acc=100.0%   train_loss=9.65e-06   test_acc=016.4%   test_loss=4.31e+00\n",
      "model #15, epoch #039:   train_acc=100.0%   train_loss=9.16e-06   test_acc=016.4%   test_loss=4.31e+00\n",
      "model #15, epoch #040:   train_acc=100.0%   train_loss=8.70e-06   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #15, epoch #041:   train_acc=100.0%   train_loss=8.27e-06   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #15, epoch #042:   train_acc=100.0%   train_loss=7.86e-06   test_acc=016.3%   test_loss=4.33e+00\n",
      "model #15, epoch #043:   train_acc=100.0%   train_loss=7.49e-06   test_acc=016.3%   test_loss=4.34e+00\n",
      "model #15, epoch #044:   train_acc=100.0%   train_loss=7.14e-06   test_acc=016.2%   test_loss=4.34e+00\n",
      "model #15, epoch #045:   train_acc=100.0%   train_loss=6.81e-06   test_acc=016.2%   test_loss=4.35e+00\n",
      "model #15, epoch #046:   train_acc=100.0%   train_loss=6.50e-06   test_acc=016.2%   test_loss=4.35e+00\n",
      "model #15, epoch #047:   train_acc=100.0%   train_loss=6.22e-06   test_acc=016.1%   test_loss=4.36e+00\n",
      "model #15, epoch #048:   train_acc=100.0%   train_loss=5.94e-06   test_acc=016.1%   test_loss=4.36e+00\n",
      "model #15, epoch #049:   train_acc=100.0%   train_loss=5.69e-06   test_acc=016.1%   test_loss=4.37e+00\n",
      "model #16, epoch #000:   train_acc=007.7%   train_loss=1.69e-02   test_acc=009.4%   test_loss=4.09e+00\n",
      "model #16, epoch #001:   train_acc=028.4%   train_loss=1.31e-02   test_acc=012.9%   test_loss=3.87e+00\n",
      "model #16, epoch #002:   train_acc=051.3%   train_loss=1.01e-02   test_acc=014.1%   test_loss=3.80e+00\n",
      "model #16, epoch #003:   train_acc=070.5%   train_loss=7.45e-03   test_acc=014.9%   test_loss=3.79e+00\n",
      "model #16, epoch #004:   train_acc=083.2%   train_loss=5.32e-03   test_acc=014.7%   test_loss=3.82e+00\n",
      "model #16, epoch #005:   train_acc=088.7%   train_loss=4.11e-03   test_acc=015.5%   test_loss=3.84e+00\n",
      "model #16, epoch #006:   train_acc=093.2%   train_loss=3.03e-03   test_acc=015.4%   test_loss=3.89e+00\n",
      "model #16, epoch #007:   train_acc=096.9%   train_loss=1.98e-03   test_acc=015.9%   test_loss=3.95e+00\n",
      "model #16, epoch #008:   train_acc=098.9%   train_loss=1.26e-03   test_acc=015.9%   test_loss=4.00e+00\n",
      "model #16, epoch #009:   train_acc=099.7%   train_loss=7.53e-04   test_acc=016.3%   test_loss=4.05e+00\n",
      "model #16, epoch #010:   train_acc=099.9%   train_loss=4.35e-04   test_acc=016.2%   test_loss=4.08e+00\n",
      "model #16, epoch #011:   train_acc=100.0%   train_loss=2.45e-04   test_acc=017.2%   test_loss=4.10e+00\n",
      "model #16, epoch #012:   train_acc=100.0%   train_loss=1.58e-04   test_acc=016.7%   test_loss=4.11e+00\n",
      "model #16, epoch #013:   train_acc=100.0%   train_loss=1.17e-04   test_acc=016.2%   test_loss=4.13e+00\n",
      "model #16, epoch #014:   train_acc=100.0%   train_loss=9.25e-05   test_acc=016.2%   test_loss=4.15e+00\n",
      "model #16, epoch #015:   train_acc=100.0%   train_loss=7.59e-05   test_acc=016.1%   test_loss=4.17e+00\n",
      "model #16, epoch #016:   train_acc=100.0%   train_loss=6.40e-05   test_acc=016.2%   test_loss=4.19e+00\n",
      "model #16, epoch #017:   train_acc=100.0%   train_loss=5.50e-05   test_acc=016.4%   test_loss=4.20e+00\n",
      "model #16, epoch #018:   train_acc=100.0%   train_loss=4.81e-05   test_acc=016.3%   test_loss=4.22e+00\n",
      "model #16, epoch #019:   train_acc=100.0%   train_loss=4.25e-05   test_acc=016.4%   test_loss=4.23e+00\n",
      "model #16, epoch #020:   train_acc=100.0%   train_loss=3.79e-05   test_acc=016.1%   test_loss=4.24e+00\n",
      "model #16, epoch #021:   train_acc=100.0%   train_loss=3.41e-05   test_acc=016.1%   test_loss=4.25e+00\n",
      "model #16, epoch #022:   train_acc=100.0%   train_loss=3.09e-05   test_acc=016.2%   test_loss=4.26e+00\n",
      "model #16, epoch #023:   train_acc=100.0%   train_loss=2.81e-05   test_acc=016.2%   test_loss=4.27e+00\n",
      "model #16, epoch #024:   train_acc=100.0%   train_loss=2.57e-05   test_acc=016.2%   test_loss=4.28e+00\n",
      "model #16, epoch #025:   train_acc=100.0%   train_loss=2.37e-05   test_acc=016.2%   test_loss=4.29e+00\n",
      "model #16, epoch #026:   train_acc=100.0%   train_loss=2.18e-05   test_acc=016.1%   test_loss=4.30e+00\n",
      "model #16, epoch #027:   train_acc=100.0%   train_loss=2.02e-05   test_acc=016.2%   test_loss=4.30e+00\n",
      "model #16, epoch #028:   train_acc=100.0%   train_loss=1.87e-05   test_acc=016.3%   test_loss=4.31e+00\n",
      "model #16, epoch #029:   train_acc=100.0%   train_loss=1.74e-05   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #16, epoch #030:   train_acc=100.0%   train_loss=1.63e-05   test_acc=016.3%   test_loss=4.33e+00\n",
      "model #16, epoch #031:   train_acc=100.0%   train_loss=1.52e-05   test_acc=016.3%   test_loss=4.33e+00\n",
      "model #16, epoch #032:   train_acc=100.0%   train_loss=1.43e-05   test_acc=016.3%   test_loss=4.34e+00\n",
      "model #16, epoch #033:   train_acc=100.0%   train_loss=1.34e-05   test_acc=016.3%   test_loss=4.35e+00\n",
      "model #16, epoch #034:   train_acc=100.0%   train_loss=1.26e-05   test_acc=016.3%   test_loss=4.36e+00\n",
      "model #16, epoch #035:   train_acc=100.0%   train_loss=1.19e-05   test_acc=016.3%   test_loss=4.36e+00\n",
      "model #16, epoch #036:   train_acc=100.0%   train_loss=1.12e-05   test_acc=016.3%   test_loss=4.37e+00\n",
      "model #16, epoch #037:   train_acc=100.0%   train_loss=1.06e-05   test_acc=016.3%   test_loss=4.38e+00\n",
      "model #16, epoch #038:   train_acc=100.0%   train_loss=1.00e-05   test_acc=016.3%   test_loss=4.38e+00\n",
      "model #16, epoch #039:   train_acc=100.0%   train_loss=9.50e-06   test_acc=016.3%   test_loss=4.39e+00\n",
      "model #16, epoch #040:   train_acc=100.0%   train_loss=9.01e-06   test_acc=016.4%   test_loss=4.39e+00\n",
      "model #16, epoch #041:   train_acc=100.0%   train_loss=8.56e-06   test_acc=016.4%   test_loss=4.40e+00\n",
      "model #16, epoch #042:   train_acc=100.0%   train_loss=8.14e-06   test_acc=016.3%   test_loss=4.41e+00\n",
      "model #16, epoch #043:   train_acc=100.0%   train_loss=7.75e-06   test_acc=016.3%   test_loss=4.41e+00\n",
      "model #16, epoch #044:   train_acc=100.0%   train_loss=7.38e-06   test_acc=016.3%   test_loss=4.42e+00\n",
      "model #16, epoch #045:   train_acc=100.0%   train_loss=7.04e-06   test_acc=016.2%   test_loss=4.42e+00\n",
      "model #16, epoch #046:   train_acc=100.0%   train_loss=6.71e-06   test_acc=016.3%   test_loss=4.43e+00\n",
      "model #16, epoch #047:   train_acc=100.0%   train_loss=6.41e-06   test_acc=016.3%   test_loss=4.44e+00\n",
      "model #16, epoch #048:   train_acc=100.0%   train_loss=6.13e-06   test_acc=016.3%   test_loss=4.44e+00\n",
      "model #16, epoch #049:   train_acc=100.0%   train_loss=5.86e-06   test_acc=016.2%   test_loss=4.45e+00\n",
      "model #17, epoch #000:   train_acc=007.5%   train_loss=1.70e-02   test_acc=009.7%   test_loss=4.07e+00\n",
      "model #17, epoch #001:   train_acc=030.6%   train_loss=1.30e-02   test_acc=013.3%   test_loss=3.85e+00\n",
      "model #17, epoch #002:   train_acc=055.4%   train_loss=9.83e-03   test_acc=014.3%   test_loss=3.77e+00\n",
      "model #17, epoch #003:   train_acc=073.8%   train_loss=7.11e-03   test_acc=014.0%   test_loss=3.77e+00\n",
      "model #17, epoch #004:   train_acc=085.4%   train_loss=5.09e-03   test_acc=014.6%   test_loss=3.79e+00\n",
      "model #17, epoch #005:   train_acc=090.9%   train_loss=3.79e-03   test_acc=014.1%   test_loss=3.83e+00\n",
      "model #17, epoch #006:   train_acc=095.7%   train_loss=2.56e-03   test_acc=014.2%   test_loss=3.88e+00\n",
      "model #17, epoch #007:   train_acc=098.4%   train_loss=1.69e-03   test_acc=014.7%   test_loss=3.96e+00\n",
      "model #17, epoch #008:   train_acc=099.5%   train_loss=1.08e-03   test_acc=015.0%   test_loss=3.99e+00\n",
      "model #17, epoch #009:   train_acc=099.9%   train_loss=6.43e-04   test_acc=015.0%   test_loss=4.05e+00\n",
      "model #17, epoch #010:   train_acc=100.0%   train_loss=3.91e-04   test_acc=015.7%   test_loss=4.10e+00\n",
      "model #17, epoch #011:   train_acc=100.0%   train_loss=2.66e-04   test_acc=015.7%   test_loss=4.13e+00\n",
      "model #17, epoch #012:   train_acc=100.0%   train_loss=1.71e-04   test_acc=015.9%   test_loss=4.15e+00\n",
      "model #17, epoch #013:   train_acc=100.0%   train_loss=1.17e-04   test_acc=015.9%   test_loss=4.17e+00\n",
      "model #17, epoch #014:   train_acc=100.0%   train_loss=8.97e-05   test_acc=015.4%   test_loss=4.19e+00\n",
      "model #17, epoch #015:   train_acc=100.0%   train_loss=7.24e-05   test_acc=015.9%   test_loss=4.20e+00\n",
      "model #17, epoch #016:   train_acc=100.0%   train_loss=5.99e-05   test_acc=015.9%   test_loss=4.21e+00\n",
      "model #17, epoch #017:   train_acc=100.0%   train_loss=5.08e-05   test_acc=015.9%   test_loss=4.22e+00\n",
      "model #17, epoch #018:   train_acc=100.0%   train_loss=4.39e-05   test_acc=016.0%   test_loss=4.23e+00\n",
      "model #17, epoch #019:   train_acc=100.0%   train_loss=3.86e-05   test_acc=016.1%   test_loss=4.25e+00\n",
      "model #17, epoch #020:   train_acc=100.0%   train_loss=3.43e-05   test_acc=016.0%   test_loss=4.26e+00\n",
      "model #17, epoch #021:   train_acc=100.0%   train_loss=3.08e-05   test_acc=015.9%   test_loss=4.27e+00\n",
      "model #17, epoch #022:   train_acc=100.0%   train_loss=2.79e-05   test_acc=015.8%   test_loss=4.28e+00\n",
      "model #17, epoch #023:   train_acc=100.0%   train_loss=2.55e-05   test_acc=015.8%   test_loss=4.29e+00\n",
      "model #17, epoch #024:   train_acc=100.0%   train_loss=2.33e-05   test_acc=015.8%   test_loss=4.30e+00\n",
      "model #17, epoch #025:   train_acc=100.0%   train_loss=2.15e-05   test_acc=016.0%   test_loss=4.31e+00\n",
      "model #17, epoch #026:   train_acc=100.0%   train_loss=1.99e-05   test_acc=016.0%   test_loss=4.32e+00\n",
      "model #17, epoch #027:   train_acc=100.0%   train_loss=1.84e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #17, epoch #028:   train_acc=100.0%   train_loss=1.72e-05   test_acc=015.8%   test_loss=4.33e+00\n",
      "model #17, epoch #029:   train_acc=100.0%   train_loss=1.60e-05   test_acc=015.8%   test_loss=4.34e+00\n",
      "model #17, epoch #030:   train_acc=100.0%   train_loss=1.50e-05   test_acc=015.8%   test_loss=4.35e+00\n",
      "model #17, epoch #031:   train_acc=100.0%   train_loss=1.41e-05   test_acc=015.8%   test_loss=4.36e+00\n",
      "model #17, epoch #032:   train_acc=100.0%   train_loss=1.32e-05   test_acc=015.8%   test_loss=4.36e+00\n",
      "model #17, epoch #033:   train_acc=100.0%   train_loss=1.24e-05   test_acc=015.9%   test_loss=4.37e+00\n",
      "model #17, epoch #034:   train_acc=100.0%   train_loss=1.17e-05   test_acc=016.0%   test_loss=4.38e+00\n",
      "model #17, epoch #035:   train_acc=100.0%   train_loss=1.11e-05   test_acc=015.8%   test_loss=4.38e+00\n",
      "model #17, epoch #036:   train_acc=100.0%   train_loss=1.05e-05   test_acc=015.9%   test_loss=4.39e+00\n",
      "model #17, epoch #037:   train_acc=100.0%   train_loss=9.94e-06   test_acc=015.9%   test_loss=4.40e+00\n",
      "model #17, epoch #038:   train_acc=100.0%   train_loss=9.43e-06   test_acc=015.9%   test_loss=4.41e+00\n",
      "model #17, epoch #039:   train_acc=100.0%   train_loss=8.95e-06   test_acc=016.0%   test_loss=4.41e+00\n",
      "model #17, epoch #040:   train_acc=100.0%   train_loss=8.51e-06   test_acc=016.0%   test_loss=4.42e+00\n",
      "model #17, epoch #041:   train_acc=100.0%   train_loss=8.10e-06   test_acc=016.1%   test_loss=4.43e+00\n",
      "model #17, epoch #042:   train_acc=100.0%   train_loss=7.71e-06   test_acc=016.0%   test_loss=4.43e+00\n",
      "model #17, epoch #043:   train_acc=100.0%   train_loss=7.35e-06   test_acc=016.0%   test_loss=4.44e+00\n",
      "model #17, epoch #044:   train_acc=100.0%   train_loss=7.02e-06   test_acc=015.9%   test_loss=4.44e+00\n",
      "model #17, epoch #045:   train_acc=100.0%   train_loss=6.70e-06   test_acc=015.9%   test_loss=4.45e+00\n",
      "model #17, epoch #046:   train_acc=100.0%   train_loss=6.40e-06   test_acc=016.0%   test_loss=4.46e+00\n",
      "model #17, epoch #047:   train_acc=100.0%   train_loss=6.12e-06   test_acc=015.9%   test_loss=4.46e+00\n",
      "model #17, epoch #048:   train_acc=100.0%   train_loss=5.86e-06   test_acc=015.9%   test_loss=4.47e+00\n",
      "model #17, epoch #049:   train_acc=100.0%   train_loss=5.61e-06   test_acc=015.9%   test_loss=4.48e+00\n",
      "model #18, epoch #000:   train_acc=008.3%   train_loss=1.68e-02   test_acc=010.3%   test_loss=4.04e+00\n",
      "model #18, epoch #001:   train_acc=030.9%   train_loss=1.28e-02   test_acc=013.2%   test_loss=3.84e+00\n",
      "model #18, epoch #002:   train_acc=054.9%   train_loss=9.73e-03   test_acc=014.5%   test_loss=3.79e+00\n",
      "model #18, epoch #003:   train_acc=074.2%   train_loss=7.05e-03   test_acc=014.9%   test_loss=3.79e+00\n",
      "model #18, epoch #004:   train_acc=085.8%   train_loss=4.99e-03   test_acc=015.3%   test_loss=3.81e+00\n",
      "model #18, epoch #005:   train_acc=091.7%   train_loss=3.63e-03   test_acc=014.8%   test_loss=3.87e+00\n",
      "model #18, epoch #006:   train_acc=096.0%   train_loss=2.51e-03   test_acc=015.0%   test_loss=3.92e+00\n",
      "model #18, epoch #007:   train_acc=098.2%   train_loss=1.74e-03   test_acc=015.3%   test_loss=3.97e+00\n",
      "model #18, epoch #008:   train_acc=099.3%   train_loss=1.17e-03   test_acc=016.1%   test_loss=4.03e+00\n",
      "model #18, epoch #009:   train_acc=099.8%   train_loss=7.07e-04   test_acc=015.6%   test_loss=4.08e+00\n",
      "model #18, epoch #010:   train_acc=100.0%   train_loss=3.87e-04   test_acc=015.5%   test_loss=4.11e+00\n",
      "model #18, epoch #011:   train_acc=100.0%   train_loss=2.27e-04   test_acc=015.8%   test_loss=4.12e+00\n",
      "model #18, epoch #012:   train_acc=100.0%   train_loss=1.52e-04   test_acc=015.8%   test_loss=4.14e+00\n",
      "model #18, epoch #013:   train_acc=100.0%   train_loss=1.10e-04   test_acc=015.9%   test_loss=4.16e+00\n",
      "model #18, epoch #014:   train_acc=100.0%   train_loss=8.57e-05   test_acc=016.1%   test_loss=4.17e+00\n",
      "model #18, epoch #015:   train_acc=100.0%   train_loss=6.99e-05   test_acc=016.5%   test_loss=4.18e+00\n",
      "model #18, epoch #016:   train_acc=100.0%   train_loss=5.87e-05   test_acc=016.4%   test_loss=4.20e+00\n",
      "model #18, epoch #017:   train_acc=100.0%   train_loss=5.08e-05   test_acc=016.3%   test_loss=4.21e+00\n",
      "model #18, epoch #018:   train_acc=100.0%   train_loss=4.76e-05   test_acc=016.3%   test_loss=4.22e+00\n",
      "model #18, epoch #019:   train_acc=100.0%   train_loss=3.97e-05   test_acc=016.2%   test_loss=4.23e+00\n",
      "model #18, epoch #020:   train_acc=100.0%   train_loss=3.62e-05   test_acc=016.2%   test_loss=4.25e+00\n",
      "model #18, epoch #021:   train_acc=100.0%   train_loss=3.22e-05   test_acc=016.2%   test_loss=4.26e+00\n",
      "model #18, epoch #022:   train_acc=100.0%   train_loss=2.86e-05   test_acc=016.3%   test_loss=4.27e+00\n",
      "model #18, epoch #023:   train_acc=100.0%   train_loss=2.57e-05   test_acc=016.2%   test_loss=4.28e+00\n",
      "model #18, epoch #024:   train_acc=100.0%   train_loss=2.35e-05   test_acc=016.1%   test_loss=4.29e+00\n",
      "model #18, epoch #025:   train_acc=100.0%   train_loss=2.16e-05   test_acc=016.1%   test_loss=4.29e+00\n",
      "model #18, epoch #026:   train_acc=100.0%   train_loss=2.00e-05   test_acc=016.1%   test_loss=4.30e+00\n",
      "model #18, epoch #027:   train_acc=100.0%   train_loss=1.86e-05   test_acc=016.1%   test_loss=4.31e+00\n",
      "model #18, epoch #028:   train_acc=100.0%   train_loss=1.73e-05   test_acc=016.0%   test_loss=4.32e+00\n",
      "model #18, epoch #029:   train_acc=100.0%   train_loss=1.61e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #18, epoch #030:   train_acc=100.0%   train_loss=1.51e-05   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #18, epoch #031:   train_acc=100.0%   train_loss=1.42e-05   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #18, epoch #032:   train_acc=100.0%   train_loss=1.33e-05   test_acc=015.9%   test_loss=4.35e+00\n",
      "model #18, epoch #033:   train_acc=100.0%   train_loss=1.25e-05   test_acc=015.9%   test_loss=4.36e+00\n",
      "model #18, epoch #034:   train_acc=100.0%   train_loss=1.18e-05   test_acc=015.9%   test_loss=4.37e+00\n",
      "model #18, epoch #035:   train_acc=100.0%   train_loss=1.12e-05   test_acc=015.9%   test_loss=4.37e+00\n",
      "model #18, epoch #036:   train_acc=100.0%   train_loss=1.05e-05   test_acc=015.9%   test_loss=4.38e+00\n",
      "model #18, epoch #037:   train_acc=100.0%   train_loss=9.99e-06   test_acc=015.9%   test_loss=4.39e+00\n",
      "model #18, epoch #038:   train_acc=100.0%   train_loss=9.47e-06   test_acc=015.9%   test_loss=4.39e+00\n",
      "model #18, epoch #039:   train_acc=100.0%   train_loss=8.99e-06   test_acc=015.9%   test_loss=4.40e+00\n",
      "model #18, epoch #040:   train_acc=100.0%   train_loss=8.55e-06   test_acc=015.9%   test_loss=4.41e+00\n",
      "model #18, epoch #041:   train_acc=100.0%   train_loss=8.13e-06   test_acc=015.9%   test_loss=4.41e+00\n",
      "model #18, epoch #042:   train_acc=100.0%   train_loss=7.74e-06   test_acc=015.9%   test_loss=4.42e+00\n",
      "model #18, epoch #043:   train_acc=100.0%   train_loss=7.37e-06   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #18, epoch #044:   train_acc=100.0%   train_loss=7.03e-06   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #18, epoch #045:   train_acc=100.0%   train_loss=6.71e-06   test_acc=015.9%   test_loss=4.44e+00\n",
      "model #18, epoch #046:   train_acc=100.0%   train_loss=6.41e-06   test_acc=015.8%   test_loss=4.45e+00\n",
      "model #18, epoch #047:   train_acc=100.0%   train_loss=6.13e-06   test_acc=015.9%   test_loss=4.45e+00\n",
      "model #18, epoch #048:   train_acc=100.0%   train_loss=5.86e-06   test_acc=015.9%   test_loss=4.46e+00\n",
      "model #18, epoch #049:   train_acc=100.0%   train_loss=5.61e-06   test_acc=015.9%   test_loss=4.46e+00\n",
      "model #19, epoch #000:   train_acc=008.1%   train_loss=1.69e-02   test_acc=011.6%   test_loss=4.02e+00\n",
      "model #19, epoch #001:   train_acc=030.0%   train_loss=1.29e-02   test_acc=014.9%   test_loss=3.81e+00\n",
      "model #19, epoch #002:   train_acc=053.1%   train_loss=9.92e-03   test_acc=015.0%   test_loss=3.74e+00\n",
      "model #19, epoch #003:   train_acc=072.5%   train_loss=7.24e-03   test_acc=015.5%   test_loss=3.74e+00\n",
      "model #19, epoch #004:   train_acc=084.9%   train_loss=5.15e-03   test_acc=015.5%   test_loss=3.77e+00\n",
      "model #19, epoch #005:   train_acc=089.9%   train_loss=3.97e-03   test_acc=015.0%   test_loss=3.82e+00\n",
      "model #19, epoch #006:   train_acc=093.7%   train_loss=2.94e-03   test_acc=016.4%   test_loss=3.87e+00\n",
      "model #19, epoch #007:   train_acc=097.3%   train_loss=1.90e-03   test_acc=016.4%   test_loss=3.94e+00\n",
      "model #19, epoch #008:   train_acc=099.2%   train_loss=1.20e-03   test_acc=015.8%   test_loss=4.00e+00\n",
      "model #19, epoch #009:   train_acc=099.8%   train_loss=7.42e-04   test_acc=015.7%   test_loss=4.03e+00\n",
      "model #19, epoch #010:   train_acc=100.0%   train_loss=4.32e-04   test_acc=015.6%   test_loss=4.06e+00\n",
      "model #19, epoch #011:   train_acc=100.0%   train_loss=2.77e-04   test_acc=015.7%   test_loss=4.08e+00\n",
      "model #19, epoch #012:   train_acc=100.0%   train_loss=1.83e-04   test_acc=015.8%   test_loss=4.10e+00\n",
      "model #19, epoch #013:   train_acc=100.0%   train_loss=1.29e-04   test_acc=015.4%   test_loss=4.11e+00\n",
      "model #19, epoch #014:   train_acc=100.0%   train_loss=9.91e-05   test_acc=016.2%   test_loss=4.13e+00\n",
      "model #19, epoch #015:   train_acc=100.0%   train_loss=7.75e-05   test_acc=015.9%   test_loss=4.15e+00\n",
      "model #19, epoch #016:   train_acc=100.0%   train_loss=6.34e-05   test_acc=015.8%   test_loss=4.17e+00\n",
      "model #19, epoch #017:   train_acc=100.0%   train_loss=5.35e-05   test_acc=015.6%   test_loss=4.18e+00\n",
      "model #19, epoch #018:   train_acc=100.0%   train_loss=4.64e-05   test_acc=015.6%   test_loss=4.19e+00\n",
      "model #19, epoch #019:   train_acc=100.0%   train_loss=4.09e-05   test_acc=015.6%   test_loss=4.21e+00\n",
      "model #19, epoch #020:   train_acc=100.0%   train_loss=3.64e-05   test_acc=015.7%   test_loss=4.22e+00\n",
      "model #19, epoch #021:   train_acc=100.0%   train_loss=3.28e-05   test_acc=015.6%   test_loss=4.23e+00\n",
      "model #19, epoch #022:   train_acc=100.0%   train_loss=2.97e-05   test_acc=015.6%   test_loss=4.24e+00\n",
      "model #19, epoch #023:   train_acc=100.0%   train_loss=2.71e-05   test_acc=015.6%   test_loss=4.25e+00\n",
      "model #19, epoch #024:   train_acc=100.0%   train_loss=2.49e-05   test_acc=015.6%   test_loss=4.26e+00\n",
      "model #19, epoch #025:   train_acc=100.0%   train_loss=2.29e-05   test_acc=015.5%   test_loss=4.27e+00\n",
      "model #19, epoch #026:   train_acc=100.0%   train_loss=2.12e-05   test_acc=015.6%   test_loss=4.27e+00\n",
      "model #19, epoch #027:   train_acc=100.0%   train_loss=1.96e-05   test_acc=015.7%   test_loss=4.28e+00\n",
      "model #19, epoch #028:   train_acc=100.0%   train_loss=1.83e-05   test_acc=015.7%   test_loss=4.29e+00\n",
      "model #19, epoch #029:   train_acc=100.0%   train_loss=1.70e-05   test_acc=015.8%   test_loss=4.30e+00\n",
      "model #19, epoch #030:   train_acc=100.0%   train_loss=1.59e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #19, epoch #031:   train_acc=100.0%   train_loss=1.49e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #19, epoch #032:   train_acc=100.0%   train_loss=1.40e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #19, epoch #033:   train_acc=100.0%   train_loss=1.32e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #19, epoch #034:   train_acc=100.0%   train_loss=1.24e-05   test_acc=015.9%   test_loss=4.34e+00\n",
      "model #19, epoch #035:   train_acc=100.0%   train_loss=1.17e-05   test_acc=015.9%   test_loss=4.34e+00\n",
      "model #19, epoch #036:   train_acc=100.0%   train_loss=1.11e-05   test_acc=015.9%   test_loss=4.35e+00\n",
      "model #19, epoch #037:   train_acc=100.0%   train_loss=1.05e-05   test_acc=016.0%   test_loss=4.36e+00\n",
      "model #19, epoch #038:   train_acc=100.0%   train_loss=9.95e-06   test_acc=016.0%   test_loss=4.36e+00\n",
      "model #19, epoch #039:   train_acc=100.0%   train_loss=9.44e-06   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #19, epoch #040:   train_acc=100.0%   train_loss=8.97e-06   test_acc=016.0%   test_loss=4.38e+00\n",
      "model #19, epoch #041:   train_acc=100.0%   train_loss=8.53e-06   test_acc=016.1%   test_loss=4.38e+00\n",
      "model #19, epoch #042:   train_acc=100.0%   train_loss=8.11e-06   test_acc=016.1%   test_loss=4.39e+00\n",
      "model #19, epoch #043:   train_acc=100.0%   train_loss=7.73e-06   test_acc=016.1%   test_loss=4.40e+00\n",
      "model #19, epoch #044:   train_acc=100.0%   train_loss=7.37e-06   test_acc=016.1%   test_loss=4.40e+00\n",
      "model #19, epoch #045:   train_acc=100.0%   train_loss=7.03e-06   test_acc=016.1%   test_loss=4.41e+00\n",
      "model #19, epoch #046:   train_acc=100.0%   train_loss=6.72e-06   test_acc=016.1%   test_loss=4.41e+00\n",
      "model #19, epoch #047:   train_acc=100.0%   train_loss=6.42e-06   test_acc=016.1%   test_loss=4.42e+00\n",
      "model #19, epoch #048:   train_acc=100.0%   train_loss=6.14e-06   test_acc=016.1%   test_loss=4.43e+00\n",
      "model #19, epoch #049:   train_acc=100.0%   train_loss=5.88e-06   test_acc=016.1%   test_loss=4.43e+00\n",
      "model #20, epoch #000:   train_acc=008.0%   train_loss=1.69e-02   test_acc=010.1%   test_loss=4.07e+00\n",
      "model #20, epoch #001:   train_acc=030.8%   train_loss=1.30e-02   test_acc=013.6%   test_loss=3.87e+00\n",
      "model #20, epoch #002:   train_acc=053.8%   train_loss=9.89e-03   test_acc=014.7%   test_loss=3.79e+00\n",
      "model #20, epoch #003:   train_acc=072.4%   train_loss=7.22e-03   test_acc=014.5%   test_loss=3.78e+00\n",
      "model #20, epoch #004:   train_acc=084.4%   train_loss=5.22e-03   test_acc=014.9%   test_loss=3.79e+00\n",
      "model #20, epoch #005:   train_acc=090.0%   train_loss=3.95e-03   test_acc=015.0%   test_loss=3.84e+00\n",
      "model #20, epoch #006:   train_acc=094.3%   train_loss=2.86e-03   test_acc=015.6%   test_loss=3.89e+00\n",
      "model #20, epoch #007:   train_acc=097.6%   train_loss=1.92e-03   test_acc=015.1%   test_loss=3.95e+00\n",
      "model #20, epoch #008:   train_acc=099.2%   train_loss=1.19e-03   test_acc=014.5%   test_loss=4.01e+00\n",
      "model #20, epoch #009:   train_acc=099.8%   train_loss=7.33e-04   test_acc=015.0%   test_loss=4.05e+00\n",
      "model #20, epoch #010:   train_acc=100.0%   train_loss=4.45e-04   test_acc=015.4%   test_loss=4.10e+00\n",
      "model #20, epoch #011:   train_acc=100.0%   train_loss=2.76e-04   test_acc=016.1%   test_loss=4.12e+00\n",
      "model #20, epoch #012:   train_acc=100.0%   train_loss=1.74e-04   test_acc=015.9%   test_loss=4.14e+00\n",
      "model #20, epoch #013:   train_acc=100.0%   train_loss=1.19e-04   test_acc=016.2%   test_loss=4.16e+00\n",
      "model #20, epoch #014:   train_acc=100.0%   train_loss=9.13e-05   test_acc=016.2%   test_loss=4.17e+00\n",
      "model #20, epoch #015:   train_acc=100.0%   train_loss=7.42e-05   test_acc=016.1%   test_loss=4.18e+00\n",
      "model #20, epoch #016:   train_acc=100.0%   train_loss=6.23e-05   test_acc=016.2%   test_loss=4.20e+00\n",
      "model #20, epoch #017:   train_acc=100.0%   train_loss=5.32e-05   test_acc=016.3%   test_loss=4.21e+00\n",
      "model #20, epoch #018:   train_acc=100.0%   train_loss=4.63e-05   test_acc=016.1%   test_loss=4.22e+00\n",
      "model #20, epoch #019:   train_acc=100.0%   train_loss=4.08e-05   test_acc=016.1%   test_loss=4.24e+00\n",
      "model #20, epoch #020:   train_acc=100.0%   train_loss=3.63e-05   test_acc=016.3%   test_loss=4.25e+00\n",
      "model #20, epoch #021:   train_acc=100.0%   train_loss=3.26e-05   test_acc=016.2%   test_loss=4.26e+00\n",
      "model #20, epoch #022:   train_acc=100.0%   train_loss=2.95e-05   test_acc=016.3%   test_loss=4.27e+00\n",
      "model #20, epoch #023:   train_acc=100.0%   train_loss=2.68e-05   test_acc=016.4%   test_loss=4.28e+00\n",
      "model #20, epoch #024:   train_acc=100.0%   train_loss=2.46e-05   test_acc=016.3%   test_loss=4.28e+00\n",
      "model #20, epoch #025:   train_acc=100.0%   train_loss=2.26e-05   test_acc=016.3%   test_loss=4.29e+00\n",
      "model #20, epoch #026:   train_acc=100.0%   train_loss=2.09e-05   test_acc=016.3%   test_loss=4.30e+00\n",
      "model #20, epoch #027:   train_acc=100.0%   train_loss=1.94e-05   test_acc=016.3%   test_loss=4.31e+00\n",
      "model #20, epoch #028:   train_acc=100.0%   train_loss=1.80e-05   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #20, epoch #029:   train_acc=100.0%   train_loss=1.68e-05   test_acc=016.2%   test_loss=4.32e+00\n",
      "model #20, epoch #030:   train_acc=100.0%   train_loss=1.57e-05   test_acc=016.4%   test_loss=4.33e+00\n",
      "model #20, epoch #031:   train_acc=100.0%   train_loss=1.47e-05   test_acc=016.3%   test_loss=4.34e+00\n",
      "model #20, epoch #032:   train_acc=100.0%   train_loss=1.38e-05   test_acc=016.3%   test_loss=4.35e+00\n",
      "model #20, epoch #033:   train_acc=100.0%   train_loss=1.30e-05   test_acc=016.4%   test_loss=4.35e+00\n",
      "model #20, epoch #034:   train_acc=100.0%   train_loss=1.22e-05   test_acc=016.4%   test_loss=4.36e+00\n",
      "model #20, epoch #035:   train_acc=100.0%   train_loss=1.16e-05   test_acc=016.4%   test_loss=4.37e+00\n",
      "model #20, epoch #036:   train_acc=100.0%   train_loss=1.09e-05   test_acc=016.4%   test_loss=4.37e+00\n",
      "model #20, epoch #037:   train_acc=100.0%   train_loss=1.03e-05   test_acc=016.3%   test_loss=4.38e+00\n",
      "model #20, epoch #038:   train_acc=100.0%   train_loss=9.81e-06   test_acc=016.4%   test_loss=4.39e+00\n",
      "model #20, epoch #039:   train_acc=100.0%   train_loss=9.31e-06   test_acc=016.4%   test_loss=4.39e+00\n",
      "model #20, epoch #040:   train_acc=100.0%   train_loss=8.84e-06   test_acc=016.4%   test_loss=4.40e+00\n",
      "model #20, epoch #041:   train_acc=100.0%   train_loss=8.41e-06   test_acc=016.3%   test_loss=4.41e+00\n",
      "model #20, epoch #042:   train_acc=100.0%   train_loss=8.01e-06   test_acc=016.4%   test_loss=4.41e+00\n",
      "model #20, epoch #043:   train_acc=100.0%   train_loss=7.63e-06   test_acc=016.4%   test_loss=4.42e+00\n",
      "model #20, epoch #044:   train_acc=100.0%   train_loss=7.28e-06   test_acc=016.4%   test_loss=4.42e+00\n",
      "model #20, epoch #045:   train_acc=100.0%   train_loss=6.95e-06   test_acc=016.4%   test_loss=4.43e+00\n",
      "model #20, epoch #046:   train_acc=100.0%   train_loss=6.64e-06   test_acc=016.4%   test_loss=4.44e+00\n",
      "model #20, epoch #047:   train_acc=100.0%   train_loss=6.34e-06   test_acc=016.4%   test_loss=4.44e+00\n",
      "model #20, epoch #048:   train_acc=100.0%   train_loss=6.07e-06   test_acc=016.4%   test_loss=4.45e+00\n",
      "model #20, epoch #049:   train_acc=100.0%   train_loss=5.81e-06   test_acc=016.3%   test_loss=4.45e+00\n",
      "model #21, epoch #000:   train_acc=008.1%   train_loss=1.69e-02   test_acc=009.8%   test_loss=4.07e+00\n",
      "model #21, epoch #001:   train_acc=031.2%   train_loss=1.29e-02   test_acc=013.1%   test_loss=3.86e+00\n",
      "model #21, epoch #002:   train_acc=054.2%   train_loss=9.84e-03   test_acc=014.7%   test_loss=3.78e+00\n",
      "model #21, epoch #003:   train_acc=073.3%   train_loss=7.14e-03   test_acc=015.8%   test_loss=3.76e+00\n",
      "model #21, epoch #004:   train_acc=085.5%   train_loss=5.08e-03   test_acc=015.2%   test_loss=3.78e+00\n",
      "model #21, epoch #005:   train_acc=091.8%   train_loss=3.70e-03   test_acc=014.8%   test_loss=3.83e+00\n",
      "model #21, epoch #006:   train_acc=094.9%   train_loss=2.76e-03   test_acc=015.4%   test_loss=3.87e+00\n",
      "model #21, epoch #007:   train_acc=097.7%   train_loss=1.85e-03   test_acc=015.9%   test_loss=3.95e+00\n",
      "model #21, epoch #008:   train_acc=099.3%   train_loss=1.16e-03   test_acc=015.4%   test_loss=4.02e+00\n",
      "model #21, epoch #009:   train_acc=099.8%   train_loss=7.23e-04   test_acc=015.0%   test_loss=4.06e+00\n",
      "model #21, epoch #010:   train_acc=099.9%   train_loss=4.46e-04   test_acc=015.4%   test_loss=4.10e+00\n",
      "model #21, epoch #011:   train_acc=100.0%   train_loss=2.74e-04   test_acc=015.9%   test_loss=4.11e+00\n",
      "model #21, epoch #012:   train_acc=100.0%   train_loss=1.70e-04   test_acc=016.1%   test_loss=4.11e+00\n",
      "model #21, epoch #013:   train_acc=100.0%   train_loss=1.19e-04   test_acc=015.6%   test_loss=4.13e+00\n",
      "model #21, epoch #014:   train_acc=100.0%   train_loss=8.97e-05   test_acc=015.7%   test_loss=4.15e+00\n",
      "model #21, epoch #015:   train_acc=100.0%   train_loss=7.16e-05   test_acc=015.6%   test_loss=4.17e+00\n",
      "model #21, epoch #016:   train_acc=100.0%   train_loss=5.96e-05   test_acc=015.7%   test_loss=4.18e+00\n",
      "model #21, epoch #017:   train_acc=100.0%   train_loss=5.11e-05   test_acc=015.8%   test_loss=4.20e+00\n",
      "model #21, epoch #018:   train_acc=100.0%   train_loss=4.46e-05   test_acc=015.7%   test_loss=4.21e+00\n",
      "model #21, epoch #019:   train_acc=100.0%   train_loss=3.95e-05   test_acc=015.7%   test_loss=4.22e+00\n",
      "model #21, epoch #020:   train_acc=100.0%   train_loss=3.54e-05   test_acc=015.6%   test_loss=4.23e+00\n",
      "model #21, epoch #021:   train_acc=100.0%   train_loss=3.19e-05   test_acc=015.7%   test_loss=4.24e+00\n",
      "model #21, epoch #022:   train_acc=100.0%   train_loss=2.90e-05   test_acc=015.7%   test_loss=4.25e+00\n",
      "model #21, epoch #023:   train_acc=100.0%   train_loss=2.65e-05   test_acc=015.7%   test_loss=4.26e+00\n",
      "model #21, epoch #024:   train_acc=100.0%   train_loss=2.43e-05   test_acc=015.8%   test_loss=4.27e+00\n",
      "model #21, epoch #025:   train_acc=100.0%   train_loss=2.24e-05   test_acc=015.7%   test_loss=4.28e+00\n",
      "model #21, epoch #026:   train_acc=100.0%   train_loss=2.07e-05   test_acc=015.9%   test_loss=4.28e+00\n",
      "model #21, epoch #027:   train_acc=100.0%   train_loss=1.92e-05   test_acc=015.9%   test_loss=4.29e+00\n",
      "model #21, epoch #028:   train_acc=100.0%   train_loss=1.79e-05   test_acc=015.9%   test_loss=4.30e+00\n",
      "model #21, epoch #029:   train_acc=100.0%   train_loss=1.67e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #21, epoch #030:   train_acc=100.0%   train_loss=1.56e-05   test_acc=016.0%   test_loss=4.31e+00\n",
      "model #21, epoch #031:   train_acc=100.0%   train_loss=1.46e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #21, epoch #032:   train_acc=100.0%   train_loss=1.37e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #21, epoch #033:   train_acc=100.0%   train_loss=1.29e-05   test_acc=016.0%   test_loss=4.33e+00\n",
      "model #21, epoch #034:   train_acc=100.0%   train_loss=1.22e-05   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #21, epoch #035:   train_acc=100.0%   train_loss=1.15e-05   test_acc=016.0%   test_loss=4.35e+00\n",
      "model #21, epoch #036:   train_acc=100.0%   train_loss=1.09e-05   test_acc=016.0%   test_loss=4.35e+00\n",
      "model #21, epoch #037:   train_acc=100.0%   train_loss=1.03e-05   test_acc=016.0%   test_loss=4.36e+00\n",
      "model #21, epoch #038:   train_acc=100.0%   train_loss=9.76e-06   test_acc=015.9%   test_loss=4.37e+00\n",
      "model #21, epoch #039:   train_acc=100.0%   train_loss=9.26e-06   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #21, epoch #040:   train_acc=100.0%   train_loss=8.79e-06   test_acc=015.9%   test_loss=4.38e+00\n",
      "model #21, epoch #041:   train_acc=100.0%   train_loss=8.36e-06   test_acc=015.9%   test_loss=4.39e+00\n",
      "model #21, epoch #042:   train_acc=100.0%   train_loss=7.96e-06   test_acc=015.9%   test_loss=4.39e+00\n",
      "model #21, epoch #043:   train_acc=100.0%   train_loss=7.58e-06   test_acc=015.9%   test_loss=4.40e+00\n",
      "model #21, epoch #044:   train_acc=100.0%   train_loss=7.23e-06   test_acc=015.9%   test_loss=4.41e+00\n",
      "model #21, epoch #045:   train_acc=100.0%   train_loss=6.90e-06   test_acc=015.8%   test_loss=4.41e+00\n",
      "model #21, epoch #046:   train_acc=100.0%   train_loss=6.59e-06   test_acc=015.8%   test_loss=4.42e+00\n",
      "model #21, epoch #047:   train_acc=100.0%   train_loss=6.29e-06   test_acc=015.8%   test_loss=4.42e+00\n",
      "model #21, epoch #048:   train_acc=100.0%   train_loss=6.02e-06   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #21, epoch #049:   train_acc=100.0%   train_loss=5.76e-06   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #22, epoch #000:   train_acc=008.0%   train_loss=1.69e-02   test_acc=009.5%   test_loss=4.08e+00\n",
      "model #22, epoch #001:   train_acc=029.2%   train_loss=1.31e-02   test_acc=012.8%   test_loss=3.87e+00\n",
      "model #22, epoch #002:   train_acc=052.5%   train_loss=1.00e-02   test_acc=013.9%   test_loss=3.79e+00\n",
      "model #22, epoch #003:   train_acc=071.9%   train_loss=7.27e-03   test_acc=014.2%   test_loss=3.78e+00\n",
      "model #22, epoch #004:   train_acc=084.3%   train_loss=5.15e-03   test_acc=014.6%   test_loss=3.81e+00\n",
      "model #22, epoch #005:   train_acc=090.1%   train_loss=3.85e-03   test_acc=014.3%   test_loss=3.86e+00\n",
      "model #22, epoch #006:   train_acc=094.8%   train_loss=2.73e-03   test_acc=014.7%   test_loss=3.91e+00\n",
      "model #22, epoch #007:   train_acc=097.8%   train_loss=1.77e-03   test_acc=015.2%   test_loss=3.97e+00\n",
      "model #22, epoch #008:   train_acc=099.2%   train_loss=1.13e-03   test_acc=015.3%   test_loss=4.00e+00\n",
      "model #22, epoch #009:   train_acc=099.8%   train_loss=6.89e-04   test_acc=015.4%   test_loss=4.04e+00\n",
      "model #22, epoch #010:   train_acc=100.0%   train_loss=4.24e-04   test_acc=015.2%   test_loss=4.08e+00\n",
      "model #22, epoch #011:   train_acc=100.0%   train_loss=2.66e-04   test_acc=014.9%   test_loss=4.11e+00\n",
      "model #22, epoch #012:   train_acc=100.0%   train_loss=1.72e-04   test_acc=015.1%   test_loss=4.15e+00\n",
      "model #22, epoch #013:   train_acc=100.0%   train_loss=1.21e-04   test_acc=015.2%   test_loss=4.17e+00\n",
      "model #22, epoch #014:   train_acc=100.0%   train_loss=9.32e-05   test_acc=015.1%   test_loss=4.19e+00\n",
      "model #22, epoch #015:   train_acc=100.0%   train_loss=7.56e-05   test_acc=015.3%   test_loss=4.20e+00\n",
      "model #22, epoch #016:   train_acc=100.0%   train_loss=6.32e-05   test_acc=015.7%   test_loss=4.22e+00\n",
      "model #22, epoch #017:   train_acc=100.0%   train_loss=5.40e-05   test_acc=015.7%   test_loss=4.23e+00\n",
      "model #22, epoch #018:   train_acc=100.0%   train_loss=4.70e-05   test_acc=015.8%   test_loss=4.24e+00\n",
      "model #22, epoch #019:   train_acc=100.0%   train_loss=4.14e-05   test_acc=015.7%   test_loss=4.25e+00\n",
      "model #22, epoch #020:   train_acc=100.0%   train_loss=3.68e-05   test_acc=015.7%   test_loss=4.26e+00\n",
      "model #22, epoch #021:   train_acc=100.0%   train_loss=3.29e-05   test_acc=015.8%   test_loss=4.28e+00\n",
      "model #22, epoch #022:   train_acc=100.0%   train_loss=2.97e-05   test_acc=015.7%   test_loss=4.29e+00\n",
      "model #22, epoch #023:   train_acc=100.0%   train_loss=2.70e-05   test_acc=015.8%   test_loss=4.30e+00\n",
      "model #22, epoch #024:   train_acc=100.0%   train_loss=2.46e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #22, epoch #025:   train_acc=100.0%   train_loss=2.26e-05   test_acc=015.7%   test_loss=4.32e+00\n",
      "model #22, epoch #026:   train_acc=100.0%   train_loss=2.08e-05   test_acc=015.6%   test_loss=4.33e+00\n",
      "model #22, epoch #027:   train_acc=100.0%   train_loss=1.92e-05   test_acc=015.6%   test_loss=4.34e+00\n",
      "model #22, epoch #028:   train_acc=100.0%   train_loss=1.78e-05   test_acc=015.6%   test_loss=4.35e+00\n",
      "model #22, epoch #029:   train_acc=100.0%   train_loss=1.66e-05   test_acc=015.6%   test_loss=4.36e+00\n",
      "model #22, epoch #030:   train_acc=100.0%   train_loss=1.55e-05   test_acc=015.6%   test_loss=4.37e+00\n",
      "model #22, epoch #031:   train_acc=100.0%   train_loss=1.45e-05   test_acc=015.6%   test_loss=4.37e+00\n",
      "model #22, epoch #032:   train_acc=100.0%   train_loss=1.36e-05   test_acc=015.7%   test_loss=4.38e+00\n",
      "model #22, epoch #033:   train_acc=100.0%   train_loss=1.28e-05   test_acc=015.6%   test_loss=4.39e+00\n",
      "model #22, epoch #034:   train_acc=100.0%   train_loss=1.20e-05   test_acc=015.6%   test_loss=4.40e+00\n",
      "model #22, epoch #035:   train_acc=100.0%   train_loss=1.13e-05   test_acc=015.6%   test_loss=4.41e+00\n",
      "model #22, epoch #036:   train_acc=100.0%   train_loss=1.07e-05   test_acc=015.6%   test_loss=4.41e+00\n",
      "model #22, epoch #037:   train_acc=100.0%   train_loss=1.01e-05   test_acc=015.5%   test_loss=4.42e+00\n",
      "model #22, epoch #038:   train_acc=100.0%   train_loss=9.62e-06   test_acc=015.5%   test_loss=4.43e+00\n",
      "model #22, epoch #039:   train_acc=100.0%   train_loss=9.13e-06   test_acc=015.5%   test_loss=4.44e+00\n",
      "model #22, epoch #040:   train_acc=100.0%   train_loss=8.67e-06   test_acc=015.6%   test_loss=4.44e+00\n",
      "model #22, epoch #041:   train_acc=100.0%   train_loss=8.24e-06   test_acc=015.6%   test_loss=4.45e+00\n",
      "model #22, epoch #042:   train_acc=100.0%   train_loss=7.85e-06   test_acc=015.6%   test_loss=4.46e+00\n",
      "model #22, epoch #043:   train_acc=100.0%   train_loss=7.48e-06   test_acc=015.5%   test_loss=4.46e+00\n",
      "model #22, epoch #044:   train_acc=100.0%   train_loss=7.13e-06   test_acc=015.5%   test_loss=4.47e+00\n",
      "model #22, epoch #045:   train_acc=100.0%   train_loss=6.81e-06   test_acc=015.5%   test_loss=4.48e+00\n",
      "model #22, epoch #046:   train_acc=100.0%   train_loss=6.50e-06   test_acc=015.4%   test_loss=4.48e+00\n",
      "model #22, epoch #047:   train_acc=100.0%   train_loss=6.22e-06   test_acc=015.4%   test_loss=4.49e+00\n",
      "model #22, epoch #048:   train_acc=100.0%   train_loss=5.95e-06   test_acc=015.5%   test_loss=4.49e+00\n",
      "model #22, epoch #049:   train_acc=100.0%   train_loss=5.69e-06   test_acc=015.5%   test_loss=4.50e+00\n",
      "model #23, epoch #000:   train_acc=008.8%   train_loss=1.68e-02   test_acc=009.7%   test_loss=4.06e+00\n",
      "model #23, epoch #001:   train_acc=032.4%   train_loss=1.28e-02   test_acc=012.2%   test_loss=3.88e+00\n",
      "model #23, epoch #002:   train_acc=055.8%   train_loss=9.66e-03   test_acc=013.6%   test_loss=3.82e+00\n",
      "model #23, epoch #003:   train_acc=074.3%   train_loss=6.95e-03   test_acc=014.5%   test_loss=3.82e+00\n",
      "model #23, epoch #004:   train_acc=086.2%   train_loss=4.89e-03   test_acc=014.4%   test_loss=3.85e+00\n",
      "model #23, epoch #005:   train_acc=092.4%   train_loss=3.52e-03   test_acc=014.5%   test_loss=3.89e+00\n",
      "model #23, epoch #006:   train_acc=095.8%   train_loss=2.60e-03   test_acc=013.8%   test_loss=3.97e+00\n",
      "model #23, epoch #007:   train_acc=098.0%   train_loss=1.79e-03   test_acc=014.7%   test_loss=4.01e+00\n",
      "model #23, epoch #008:   train_acc=099.3%   train_loss=1.14e-03   test_acc=014.7%   test_loss=4.07e+00\n",
      "model #23, epoch #009:   train_acc=099.7%   train_loss=6.88e-04   test_acc=015.0%   test_loss=4.12e+00\n",
      "model #23, epoch #010:   train_acc=100.0%   train_loss=4.07e-04   test_acc=015.5%   test_loss=4.14e+00\n",
      "model #23, epoch #011:   train_acc=100.0%   train_loss=2.43e-04   test_acc=015.1%   test_loss=4.15e+00\n",
      "model #23, epoch #012:   train_acc=100.0%   train_loss=1.57e-04   test_acc=015.6%   test_loss=4.18e+00\n",
      "model #23, epoch #013:   train_acc=100.0%   train_loss=1.10e-04   test_acc=015.9%   test_loss=4.19e+00\n",
      "model #23, epoch #014:   train_acc=100.0%   train_loss=8.54e-05   test_acc=015.8%   test_loss=4.21e+00\n",
      "model #23, epoch #015:   train_acc=100.0%   train_loss=6.92e-05   test_acc=015.9%   test_loss=4.23e+00\n",
      "model #23, epoch #016:   train_acc=100.0%   train_loss=5.80e-05   test_acc=016.1%   test_loss=4.25e+00\n",
      "model #23, epoch #017:   train_acc=100.0%   train_loss=4.96e-05   test_acc=015.9%   test_loss=4.26e+00\n",
      "model #23, epoch #018:   train_acc=100.0%   train_loss=4.32e-05   test_acc=015.9%   test_loss=4.28e+00\n",
      "model #23, epoch #019:   train_acc=100.0%   train_loss=3.82e-05   test_acc=015.8%   test_loss=4.29e+00\n",
      "model #23, epoch #020:   train_acc=100.0%   train_loss=3.41e-05   test_acc=015.8%   test_loss=4.30e+00\n",
      "model #23, epoch #021:   train_acc=100.0%   train_loss=3.07e-05   test_acc=015.9%   test_loss=4.31e+00\n",
      "model #23, epoch #022:   train_acc=100.0%   train_loss=2.79e-05   test_acc=015.9%   test_loss=4.32e+00\n",
      "model #23, epoch #023:   train_acc=100.0%   train_loss=2.55e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #23, epoch #024:   train_acc=100.0%   train_loss=2.34e-05   test_acc=015.8%   test_loss=4.34e+00\n",
      "model #23, epoch #025:   train_acc=100.0%   train_loss=2.16e-05   test_acc=015.8%   test_loss=4.35e+00\n",
      "model #23, epoch #026:   train_acc=100.0%   train_loss=2.00e-05   test_acc=015.8%   test_loss=4.36e+00\n",
      "model #23, epoch #027:   train_acc=100.0%   train_loss=1.85e-05   test_acc=015.7%   test_loss=4.37e+00\n",
      "model #23, epoch #028:   train_acc=100.0%   train_loss=1.72e-05   test_acc=015.7%   test_loss=4.38e+00\n",
      "model #23, epoch #029:   train_acc=100.0%   train_loss=1.61e-05   test_acc=015.8%   test_loss=4.39e+00\n",
      "model #23, epoch #030:   train_acc=100.0%   train_loss=1.51e-05   test_acc=015.8%   test_loss=4.40e+00\n",
      "model #23, epoch #031:   train_acc=100.0%   train_loss=1.41e-05   test_acc=015.8%   test_loss=4.40e+00\n",
      "model #23, epoch #032:   train_acc=100.0%   train_loss=1.33e-05   test_acc=015.8%   test_loss=4.41e+00\n",
      "model #23, epoch #033:   train_acc=100.0%   train_loss=1.25e-05   test_acc=015.9%   test_loss=4.42e+00\n",
      "model #23, epoch #034:   train_acc=100.0%   train_loss=1.18e-05   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #23, epoch #035:   train_acc=100.0%   train_loss=1.11e-05   test_acc=015.9%   test_loss=4.43e+00\n",
      "model #23, epoch #036:   train_acc=100.0%   train_loss=1.05e-05   test_acc=015.9%   test_loss=4.44e+00\n",
      "model #23, epoch #037:   train_acc=100.0%   train_loss=9.96e-06   test_acc=015.9%   test_loss=4.45e+00\n",
      "model #23, epoch #038:   train_acc=100.0%   train_loss=9.44e-06   test_acc=016.0%   test_loss=4.46e+00\n",
      "model #23, epoch #039:   train_acc=100.0%   train_loss=8.96e-06   test_acc=015.9%   test_loss=4.46e+00\n",
      "model #23, epoch #040:   train_acc=100.0%   train_loss=8.51e-06   test_acc=015.9%   test_loss=4.47e+00\n",
      "model #23, epoch #041:   train_acc=100.0%   train_loss=8.09e-06   test_acc=016.0%   test_loss=4.48e+00\n",
      "model #23, epoch #042:   train_acc=100.0%   train_loss=7.71e-06   test_acc=016.0%   test_loss=4.48e+00\n",
      "model #23, epoch #043:   train_acc=100.0%   train_loss=7.34e-06   test_acc=016.0%   test_loss=4.49e+00\n",
      "model #23, epoch #044:   train_acc=100.0%   train_loss=7.00e-06   test_acc=016.0%   test_loss=4.50e+00\n",
      "model #23, epoch #045:   train_acc=100.0%   train_loss=6.68e-06   test_acc=016.0%   test_loss=4.50e+00\n",
      "model #23, epoch #046:   train_acc=100.0%   train_loss=6.38e-06   test_acc=016.0%   test_loss=4.51e+00\n",
      "model #23, epoch #047:   train_acc=100.0%   train_loss=6.10e-06   test_acc=016.0%   test_loss=4.52e+00\n",
      "model #23, epoch #048:   train_acc=100.0%   train_loss=5.83e-06   test_acc=016.0%   test_loss=4.52e+00\n",
      "model #23, epoch #049:   train_acc=100.0%   train_loss=5.58e-06   test_acc=015.9%   test_loss=4.53e+00\n",
      "model #24, epoch #000:   train_acc=007.7%   train_loss=1.69e-02   test_acc=009.5%   test_loss=4.07e+00\n",
      "model #24, epoch #001:   train_acc=029.9%   train_loss=1.30e-02   test_acc=013.7%   test_loss=3.85e+00\n",
      "model #24, epoch #002:   train_acc=053.5%   train_loss=9.99e-03   test_acc=015.1%   test_loss=3.77e+00\n",
      "model #24, epoch #003:   train_acc=072.5%   train_loss=7.29e-03   test_acc=015.5%   test_loss=3.75e+00\n",
      "model #24, epoch #004:   train_acc=084.8%   train_loss=5.19e-03   test_acc=014.8%   test_loss=3.78e+00\n",
      "model #24, epoch #005:   train_acc=091.0%   train_loss=3.84e-03   test_acc=015.7%   test_loss=3.84e+00\n",
      "model #24, epoch #006:   train_acc=095.4%   train_loss=2.71e-03   test_acc=015.3%   test_loss=3.89e+00\n",
      "model #24, epoch #007:   train_acc=097.9%   train_loss=1.85e-03   test_acc=015.2%   test_loss=3.93e+00\n",
      "model #24, epoch #008:   train_acc=099.3%   train_loss=1.19e-03   test_acc=015.5%   test_loss=3.99e+00\n",
      "model #24, epoch #009:   train_acc=099.8%   train_loss=7.27e-04   test_acc=015.1%   test_loss=4.04e+00\n",
      "model #24, epoch #010:   train_acc=100.0%   train_loss=4.24e-04   test_acc=015.5%   test_loss=4.08e+00\n",
      "model #24, epoch #011:   train_acc=100.0%   train_loss=2.45e-04   test_acc=015.5%   test_loss=4.11e+00\n",
      "model #24, epoch #012:   train_acc=100.0%   train_loss=1.66e-04   test_acc=016.0%   test_loss=4.12e+00\n",
      "model #24, epoch #013:   train_acc=100.0%   train_loss=1.22e-04   test_acc=016.0%   test_loss=4.14e+00\n",
      "model #24, epoch #014:   train_acc=100.0%   train_loss=9.60e-05   test_acc=015.8%   test_loss=4.15e+00\n",
      "model #24, epoch #015:   train_acc=100.0%   train_loss=7.90e-05   test_acc=015.8%   test_loss=4.17e+00\n",
      "model #24, epoch #016:   train_acc=100.0%   train_loss=6.68e-05   test_acc=015.7%   test_loss=4.18e+00\n",
      "model #24, epoch #017:   train_acc=100.0%   train_loss=5.74e-05   test_acc=015.7%   test_loss=4.20e+00\n",
      "model #24, epoch #018:   train_acc=100.0%   train_loss=4.99e-05   test_acc=015.8%   test_loss=4.21e+00\n",
      "model #24, epoch #019:   train_acc=100.0%   train_loss=4.38e-05   test_acc=015.9%   test_loss=4.23e+00\n",
      "model #24, epoch #020:   train_acc=100.0%   train_loss=3.88e-05   test_acc=016.0%   test_loss=4.24e+00\n",
      "model #24, epoch #021:   train_acc=100.0%   train_loss=3.46e-05   test_acc=015.9%   test_loss=4.25e+00\n",
      "model #24, epoch #022:   train_acc=100.0%   train_loss=3.11e-05   test_acc=016.1%   test_loss=4.26e+00\n",
      "model #24, epoch #023:   train_acc=100.0%   train_loss=2.81e-05   test_acc=015.8%   test_loss=4.27e+00\n",
      "model #24, epoch #024:   train_acc=100.0%   train_loss=2.55e-05   test_acc=015.8%   test_loss=4.28e+00\n",
      "model #24, epoch #025:   train_acc=100.0%   train_loss=2.33e-05   test_acc=015.9%   test_loss=4.29e+00\n",
      "model #24, epoch #026:   train_acc=100.0%   train_loss=2.14e-05   test_acc=015.7%   test_loss=4.30e+00\n",
      "model #24, epoch #027:   train_acc=100.0%   train_loss=1.97e-05   test_acc=015.8%   test_loss=4.31e+00\n",
      "model #24, epoch #028:   train_acc=100.0%   train_loss=1.82e-05   test_acc=016.0%   test_loss=4.32e+00\n",
      "model #24, epoch #029:   train_acc=100.0%   train_loss=1.69e-05   test_acc=015.9%   test_loss=4.33e+00\n",
      "model #24, epoch #030:   train_acc=100.0%   train_loss=1.57e-05   test_acc=015.9%   test_loss=4.34e+00\n",
      "model #24, epoch #031:   train_acc=100.0%   train_loss=1.47e-05   test_acc=016.0%   test_loss=4.34e+00\n",
      "model #24, epoch #032:   train_acc=100.0%   train_loss=1.37e-05   test_acc=015.9%   test_loss=4.35e+00\n",
      "model #24, epoch #033:   train_acc=100.0%   train_loss=1.29e-05   test_acc=016.0%   test_loss=4.36e+00\n",
      "model #24, epoch #034:   train_acc=100.0%   train_loss=1.21e-05   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #24, epoch #035:   train_acc=100.0%   train_loss=1.14e-05   test_acc=016.0%   test_loss=4.37e+00\n",
      "model #24, epoch #036:   train_acc=100.0%   train_loss=1.08e-05   test_acc=016.0%   test_loss=4.38e+00\n",
      "model #24, epoch #037:   train_acc=100.0%   train_loss=1.02e-05   test_acc=016.0%   test_loss=4.39e+00\n",
      "model #24, epoch #038:   train_acc=100.0%   train_loss=9.64e-06   test_acc=016.1%   test_loss=4.39e+00\n",
      "model #24, epoch #039:   train_acc=100.0%   train_loss=9.14e-06   test_acc=016.1%   test_loss=4.40e+00\n",
      "model #24, epoch #040:   train_acc=100.0%   train_loss=8.67e-06   test_acc=016.0%   test_loss=4.41e+00\n",
      "model #24, epoch #041:   train_acc=100.0%   train_loss=8.24e-06   test_acc=016.0%   test_loss=4.41e+00\n",
      "model #24, epoch #042:   train_acc=100.0%   train_loss=7.84e-06   test_acc=015.9%   test_loss=4.42e+00\n",
      "model #24, epoch #043:   train_acc=100.0%   train_loss=7.47e-06   test_acc=016.1%   test_loss=4.43e+00\n",
      "model #24, epoch #044:   train_acc=100.0%   train_loss=7.12e-06   test_acc=016.0%   test_loss=4.43e+00\n",
      "model #24, epoch #045:   train_acc=100.0%   train_loss=6.79e-06   test_acc=016.0%   test_loss=4.44e+00\n",
      "model #24, epoch #046:   train_acc=100.0%   train_loss=6.49e-06   test_acc=015.9%   test_loss=4.45e+00\n",
      "model #24, epoch #047:   train_acc=100.0%   train_loss=6.20e-06   test_acc=016.0%   test_loss=4.45e+00\n",
      "model #24, epoch #048:   train_acc=100.0%   train_loss=5.93e-06   test_acc=016.0%   test_loss=4.46e+00\n",
      "model #24, epoch #049:   train_acc=100.0%   train_loss=5.67e-06   test_acc=016.0%   test_loss=4.46e+00\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    PytorchModel(\n",
    "        model_obj=train_model(get_model(), k, 'cuda:0'),\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "    for k in range(num_reference_models + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dataset* object is not needed anymore, let's delete it to free up some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine the models and their associated training subsets into *InformationSource* from *privacy-meter*. *InformationSource* is a fundamental building block of the library, from which the auditing algorithms get all the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "hRKoCOo1KE-S",
    "outputId": "a2d7abea-719c-4c7e-c552-005bd3233ee9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_info_source = InformationSource(\n",
    "    models=[models[0]],\n",
    "    datasets=[datasets_list[0]]\n",
    ")\n",
    "\n",
    "reference_info_source = InformationSource(\n",
    "    models=models[1:],\n",
    "    datasets=datasets_list[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can choose which one of the built-in metric we want to use (or define our own), and define an auditing strategy based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sO8kELAEKJ4D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = ReferenceMetric(\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    signals=[ModelLoss()],\n",
    "    hypothesis_test_func=gaussian_threshold_func,\n",
    "    logs_dirname='./demo-logs'\n",
    ")\n",
    "\n",
    "audit_obj = Audit(\n",
    "    metrics=metric,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    target_info_sources=target_info_source,\n",
    "    reference_info_sources=reference_info_source,\n",
    "    fpr_tolerances=fpr_tolerance_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to actually run the audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eDFdVwuqKLuY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are stored in: ['/mnt/backup1/home/victor/log_2022-11-17_13-20-16-000']\n"
     ]
    }
   ],
   "source": [
    "audit_obj.prepare()\n",
    "audit_results = audit_obj.run()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the demo, we can ask the *privacy-meter* library to plot to ROC curve based on the audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_report.REPORT_FILES_DIR = '/home/victor/backup/ml_privacy_meter/privacy_meter/report_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROCCurveReport.generate_report(\n",
    "    metric_result=audit_results,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    show=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
